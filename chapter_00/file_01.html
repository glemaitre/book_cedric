
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Famille des modèles paramétriques &#8212; Cédric&#39;s book</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Famille des modèles non-paramétriques" href="file_02.html" />
    <link rel="prev" title="Introduction" href="file_00.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Cédric's book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../landing-page.html">
   Table of content
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="chapter_00_intro.html">
   Apprentisage automatique supervisé
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="file_00.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Famille des modèles paramétriques
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="file_02.html">
     Famille des modèles non-paramétriques
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="file_03.html">
     Recherche des hyperparamètres
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="file_04.html">
     Gestion des variables catégorielles
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter_99/chapter_99_intro.html">
   Real-world examples
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter_99/file_00.html">
     Analyse de statistique tradionnelle
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/chapter_00/file_01.py"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.py</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Famille des modèles paramétriques
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modele-lineaire">
     Modèle linéaire
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#trouver-le-meilleur-modele-possible">
       Trouver le meilleur modèle possible
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#markdown">
   %% [markdown]
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#we-observe-that-there-is-a-reasonable-linear-relationship-between-the-flipper">
   We observe that there is a reasonable linear relationship between the flipper
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#length-and-the-body-mass-here-our-target-to-be-predicted-will-be-the-body">
   length and the body mass. Here, our target to be predicted will be the body
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mass-while-the-flipper-length-will-be-a-feature">
   mass while the flipper length will be a feature.
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   %% [markdown]
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#in-the-previous-notebook-we-used-a-tt-linearregression-tt-from-scikit-learn-and">
   In the previous notebook, we used a
   <tt>
    LinearRegression
   </tt>
   from scikit-learn and
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#show-that-we-could-learn-the-state-of-the-model-from-the-data-when-calling">
   show that we could learn the state of the model from the data when calling
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tt-fit-tt-and-use-these-states-for-prediction-when-calling-the-method">
   <tt>
    fit
   </tt>
   and use these states for prediction when calling the method
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tt-predict-tt">
   <tt>
    predict
   </tt>
   .
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   %% [markdown]
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id9">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#in-the-previous-notebook-we-quickly-mentioned-that-the-linear-regression-model-was">
   In the previous notebook, we quickly mentioned that the linear regression model was
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#minizing-an-error-between-the-true-target-and-the-predicted-target-this-error-is-also">
   minizing an error between the true target and the predicted target. This error is also
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#known-as-loss-function-the-loss-that-is-minimized-in-this-case-is-known-as-the-least">
   known as loss function. The loss that is minimized in this case is known as the least
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#squared-error-this-loss-is-defined-as">
   squared error. This loss is defined as:
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id10">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id11">
   $$
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loss-y-hat-y-2">
   loss = (y - \hat{y})^2
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id12">
   $$
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id13">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#that-is">
   that is
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id14">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id15">
   $$
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loss-y-x-beta-2">
   loss = (y - X \beta)^2
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id16">
   $$
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id17">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#we-can-check-what-the-loss-look-likes-in-practice">
   We can check what the loss look likes in practice:
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id18">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id19">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id20">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id21">
   %% [markdown]
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id22">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#looking-at-the-shape-of-the-loss-function-we-see-that-the-bell-shape-of-the-loss-will">
   Looking at the shape of the loss function, we see that the bell shape of the loss will
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#impact-greatly-the-large-error-in-practice-this-will-have-an-impact-on-the-fit">
   impact greatly the large error. In practice, this will have an impact on the fit.
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id23">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id24">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id25">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id26">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id27">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id28">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id29">
   %% [markdown]
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id30">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#instead-of-using-the-squared-loss-we-will-use-a-loss-known-as-the-huber-loss-in-this">
   Instead of using the squared loss, we will use a loss known as the Huber loss. In this
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regard-we-will-use-the-huberregressor-model-available-in-scikit-learn-we-will-fit">
   regard, we will use the HuberRegressor model available in scikit-learn. We will fit
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#this-model-in-the-exact-similar-way-that-we-previously-did">
   this model in the exact similar way that we previously did.
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id31">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id32">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id33">
   %% [markdown]
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id34">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#we-observe-that-the-outlier-has-much-less-weight-than-in-the-case-of-the-least-squared">
   We observe that the outlier has much less weight than in the case of the least squared
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loss">
   loss.
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id35">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id36">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id37">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id38">
   %% [markdown]
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id39">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#we-observe-that-the-huber-and-absolute-losses-are-penalizing-less-outliers-it-means">
   We observe that the Huber and absolute losses are penalizing less outliers. It means
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#that-these-outliers-will-be-less-attractive-and-we-will-not-try-to-find-beta-that">
   that these outliers will be less attractive and we will not try to find
   <span class="math notranslate nohighlight">
    \(\beta\)
   </span>
   that
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#try-to-minimize-this-large-error-indeed-the-tt-huberregressor-tt-will-give-an">
   try to minimize this large error. Indeed, the
   <tt>
    HuberRegressor
   </tt>
   will give an
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimator-of-the-median-instead-of-the-mean">
   estimator of the median instead of the mean.
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id40">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#if-one-is-interesting-in-other-quantile-than-the-median-scikit-learn-provides-an">
   If one is interesting in other quantile than the median, scikit-learn provides an
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimator-called-quantileregressor-that-minimizes-the-pinball-loss-and-provide-a">
   estimator called
   <code class="docutils literal notranslate">
    <span class="pre">
     QuantileRegressor
    </span>
   </code>
   that minimizes the pinball loss and provide a
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimator-of-the-requested-quantile-for-instance-one-could-request-the-median-in-the">
   estimator of the requested quantile. For instance, one could request the median in the
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#following-manner">
   following manner:
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id41">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id42">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id43">
   %% [markdown]
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id44">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#principe-de-la-regularisation">
   ### Principe de la régularisation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id45">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#in-this-first-example-we-will-show-a-known-issue-due-to-correlated-features-when">
   In this first example, we will show a known issue due to correlated features when
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-a-linear-model">
   fitting a linear model.
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id46">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-data-generative-process-to-create-the-data-is-a-linear-relationship-between-the">
   The data generative process to create the data is a linear relationship between the
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#features-and-the-target-however-out-of-5-features-only-2-features-will-be-used">
   features and the target. However, out of 5 features, only 2 features will be used
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#while-3-other-features-will-not-be-linked-to-the-target-in-addition-a-little-bit-of">
   while 3 other features will not be linked to the target. In addition, a little bit of
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#noise-will-be-added-when-generating-the-dataset-we-can-as-well-get-the-true-model">
   noise will be added. When generating the dataset, we can as well get the true model.
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id47">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id48">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id49">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id50">
   %% [markdown]
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id51">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#plotting-the-true-coefficients-we-observe-that-only-2-features-out-of-the-5-features">
   Plotting the true coefficients, we observe that only 2 features out of the 5 features
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#as-an-impact-on-the-target">
   as an impact on the target.
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id52">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#now-we-will-fit-a-linear-model-on-this-dataset">
   Now, we will fit a linear model on this dataset.
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id53">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id54">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id55">
   %% [markdown]
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id56">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#we-observe-that-we-can-recover-almost-the-true-coefficients-the-small-fluctuation-are">
   We observe that we can recover almost the true coefficients. The small fluctuation are
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#due-to-the-noise-that-we-added-into-the-dataset-when-generating-it">
   due to the noise that we added into the dataset when generating it.
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id57">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id58">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id59">
   %%
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id60">
   %% [markdown]
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id61">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id62">
   %% [markdown]
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id63">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#importance-du-pretraitement">
   ### Importance du prétraitement
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id64">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#alternative-a-la-minimization-moindres-carres">
   ### Alternative à la minimization moindres carrés
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id65">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resolution-de-problemes-non-lineaires">
   ## Résolution de problèmes non-linéaires
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id66">
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quantifier-l-incertitude-des-predictions">
   ## Quantifier l’incertitude des prédictions
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="famille-des-modeles-parametriques">
<h1>Famille des modèles paramétriques<a class="headerlink" href="#famille-des-modeles-parametriques" title="Permalink to this headline">¶</a></h1>
<p>Dans le chapitre précédent, nous avons détaillé le principe d’un modèle
prédictif de manière mathématique. Nous pouvons rappeler cette formulation :</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f(X)
\]</div>
<p>Nous avons même donné un exemple d’un modèle assez naif ou nous avons utilisé
une relation entre notre variable d’entrée et notre variable de sortie. Cette
manière de définir un modèle est nommée <strong>modèle paramétrique</strong>. En effet,
nous avons défini un modèle paramétrique de la forme suivante :</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f(X) = X \beta
\]</div>
<p>Le paramètre <span class="math notranslate nohighlight">\(\beta\)</span> est donc le <strong>paramètre</strong> de notre modèle. L’idée
derrière cette famille de modèles est donc de pouvoir compresser
l’information de notre jeu de données d’apprentissage avec seulement quelques
paramètres et un <em>apriori</em> sur la relation entre <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(y\)</span> (nous reviendrons
plus en détail sur cet aspect dans les sections qui viennent).</p>
<p>Dans ce chapitre, nous allons tout d’abord détailler une des familles les
plus simple : les modèles linéaires. Nous présenterons certaines composantes
importantes de ce type de modèle. Par la suite, nous montrerons que ces
modèles peuvent également utilisés pour des problèmes non-linéaires.</p>
<div class="section" id="modele-lineaire">
<h2>Modèle linéaire<a class="headerlink" href="#modele-lineaire" title="Permalink to this headline">¶</a></h2>
<p>Un modèle linéaire est un modèle paramétrique qui est défini par une relation
entre <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(y\)</span> tel que <span class="math notranslate nohighlight">\(y\)</span> est une combination linéaire de <span class="math notranslate nohighlight">\(X\)</span>. Notre
modèle dans le chapitre précédent était un modèle linéaire :</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f(X, \beta) = X \beta
\]</div>
<p>Le terme “combination” nous indique que nous pouvons généraliser cette
relation en combinant toutes les variables d’entrée (i.e. colonnes) de <span class="math notranslate nohighlight">\(X\)</span>.
Un tel modèle est donc défini de la manière suivante :</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f(X, \beta) = X \beta = \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n
X_n
\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta_n\)</span> est le paramètre associé à la variable <span class="math notranslate nohighlight">\(X_n\)</span>. La relation
ci-dessus force notre modèle de prédire 0 lorsque les valeurs dans <span class="math notranslate nohighlight">\(X\)</span> sont
également à 0. Pour avoir plus de flexibilité, un paramètre <span class="math notranslate nohighlight">\(\beta_0\)</span> est
utilisé pour représenter cette constante et est appelé l’<strong>intercept</strong>.</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f(X, \beta) = X \beta = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... +
\beta_n X_n
\]</div>
<p>Nous pouvons donc comprendre que nous faisons un <em>apriori</em> entre le lien
entre <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(y\)</span> : nous pensons que <span class="math notranslate nohighlight">\(y\)</span> est une combinaison linéaire de <span class="math notranslate nohighlight">\(X\)</span>
et que cet relations est suffisante. Un peu plus tard dans ce chapitre, nous
donnerons des exemples où ce n’est pas le cas et où nous devrons modifier la
formulation de notre modèle.</p>
<p>En revanche, si cet <em>apriori</em> est correct, nous venons donc de compresser
notre de dataset de taille composé de <span class="math notranslate nohighlight">\(N\)</span> échantillons à un modèle de taille
<span class="math notranslate nohighlight">\(P + 1\)</span> paramètres (i.e. + 1 corresponds à l’intercept). Maintenant que nous
avons défini notre modèle, il nous est possible de trouver les paramètres.</p>
<div class="section" id="trouver-le-meilleur-modele-possible">
<h3>Trouver le meilleur modèle possible<a class="headerlink" href="#trouver-le-meilleur-modele-possible" title="Permalink to this headline">¶</a></h3>
<p>Maintenant que nous connaisons la paramétrisation de notre modèle, nous
pouvons l’illustrer sur le même jeu de données que nous avons utilisé dans
le chapitre précédent. Tout d’abord, nous chargeons les données.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">donnees</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../datasets/penguins_regression.csv&quot;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">donnees</span><span class="p">[[</span><span class="s2">&quot;Longueur Aileron (mm)&quot;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">donnees</span><span class="p">[</span><span class="s2">&quot;Masse Corporelle (g)&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Et nous pouvons visualaliser la relation entre <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(y\)</span> :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;poster&quot;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Masse corporelle en fonction de</span><span class="se">\n</span><span class="s2">la longueur d&#39;aileron&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_3_0.png" src="../_images/file_01_3_0.png" />
</div>
</div>
<p>Il existe une infinité de modèle linéaire qui pourraient être utilisés pour
pour prédire la masse corporelle de nos pingouins. Définissons une fonction
Python générique qui permet de prédire la masse corporelle de notre pinguoin
en fonction de la longueur d’aileron.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">modele_lineaire</span><span class="p">(</span><span class="n">longueur_aileron</span><span class="p">,</span> <span class="n">parametres</span><span class="p">):</span>
    <span class="c1"># notre modèle est défini par: y = beta_0 + x_1 * beta_1</span>
    <span class="k">return</span> <span class="n">parametres</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">parametres</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">longueur_aileron</span>
</pre></div>
</div>
</div>
</div>
<p>Maintenant que nous avons notre modèle, nous pouvons visualiser quelques
modèles avec différents paramètres.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions_modele_1</span> <span class="o">=</span> <span class="n">modele_lineaire</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">3_000</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
<span class="n">predictions_modele_2</span> <span class="o">=</span> <span class="n">modele_lineaire</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">6_000</span><span class="p">,</span> <span class="mi">50</span><span class="p">])</span>
<span class="n">predictions_modele_3</span> <span class="o">=</span> <span class="n">modele_lineaire</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">2_000</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_modele_1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Modèle #1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_modele_2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Modèle #2&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_modele_3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Modèle #3&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Quel modèle est le meilleur?&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_8_0.png" src="../_images/file_01_8_0.png" />
</div>
</div>
<p>A partir de ce graphique, la question que nous pourrions avoir est de savoir
quel modèle est le meilleur. Qualitativement, nous pourrions dire que le
modèle #1 est le pire modèle. Entre le modèle #2 et #3, nous pourrions
préviligier le modèle #2 car il semble plus “centré” avec nos données.</p>
<p>Cependant, choisir un modèle ne peut-être basé sur une évaluation
qualitative. Dans le chapitre précédent, nous avons utilisé différentes
méthodes qui calculaient une erreur. Nous pouvons ici calculer une erreur
donnée : l’erreur quadratique moyenne.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Erreur du modèle #1: </span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_modele_1</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Erreur du modèle #2: </span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_modele_2</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Erreur du modèle #3: </span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_modele_3</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Erreur du modèle #1: 1609923.83
Erreur du modèle #2: 178899.85
Erreur du modèle #3: 261327.34
</pre></div>
</div>
</div>
</div>
<p>En utilisant cette erreur, nous avons la confirmation que le modèle #2 a la
plus petite erreur. En revanche, est ce que ce modèle est le meilleur
possible ? Si non, comment pouvons nous trouver un modèle la plus faible
possible ?</p>
<p>Nous donc un probème d’optimisation où nous voudrions minimiser cette erreur
également appelée <strong>fonction de coût</strong> dans ce contexte. Donc, nous pouvons
donc définir notre fonctionde coup comme l’erreur quadratique moyenne
formulée ci-dessous :</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\beta) = \frac{1}{N} \sum_{i=1}^N \left( y_i - f(X_i, \beta)
\right)^2
\]</div>
<p>Et nour chercherons donc à minimiser cette fonction de coût. En d’autre
termes, nous serions intéressés par trouver le minimum de
<span class="math notranslate nohighlight">\(\mathcal{L}(\beta)\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\min_{\beta} \mathcal{L}(\beta)
\]</div>
<p>Trouver le minimum d’une fonction donnée est un problème typique
d’<strong>optimisation methématique</strong> et il existe plusieurs méthodes, certaines
plus performantes que d’autres, dépendant de la fonction à minimiser. Nous
pouvons mentionner les méthodes basées sur le gradient qui nécessitent de
pouvoir dériver la fonction de coût.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Il existe une solution analytique pour la fonction de coût que nous avons
définie.</p>
<div class="math notranslate nohighlight">
\[
\beta = \left( X^T X \right)^{-1} X^T y
\]</div>
<p>En revanche, nous avons introduit la méthode de gradient car elle nous
permettra d’avoir une certaine réflexion concernant les futurs fonctions de
coût que nous allons définir.</p>
</div>
<p>Nous avons la chance que notre fonction de coût définie comme l’erreur
quadratique moyenne soit facilement dérivable. Il serait donc facile de
calculer le mimimum de cette fonction de coût.</p>
<p><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> nous propose une classe dénommée <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> qui
permet de minimser cette fonction de coût. Nous allons la mettre en pratique
dès maintenant.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">modele</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">modele</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearRegression()
</pre></div>
</div>
</div>
</div>
<p>Maintenant que notre modèle est entrainé, nous pouvons l’utiliser pour
observer visuellement quels sont les prédictions produites par ce modèle.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">modele</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Regression lineaire&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Modele LinearRegression&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_15_0.png" src="../_images/file_01_15_0.png" />
</div>
</div>
<p>Ce modèle semble donc bien minimiser la fonction de coût et est
qualititivement correct. Nous pouvons donc maintenant nous pencher sur notre
modèle et obtenir la valeurs des paramètres.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modele</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([49.68556641])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modele</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-5780.831358077066
</pre></div>
</div>
</div>
</div>
<p>Nous pouvons donc observer deux attributs qui correspondent aux paramètres de
notre modèle. <code class="docutils literal notranslate"><span class="pre">coef_</span></code> contient les paramètres de <span class="math notranslate nohighlight">\(\beta_1, ..., \beta_n\)</span>
alors que <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> contient le paramètre de <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Dans scikit-learn, les attributs finissant par <code class="docutils literal notranslate"><span class="pre">_</span></code> sont des attributs
qui sont créés après avoir appelé la méthode <code class="docutils literal notranslate"><span class="pre">fit()</span></code>. Ils sont liés à
l’algorithme d’apprentissage et seront nécessaires pour pouvoir créer
des prédictions.</p>
</div>
<p>Maintenant, nous pouvons interpréter la valeurs des paramètres de notre
modèle. La valeur dans la variable <code class="docutils literal notranslate"><span class="pre">coef_</span></code> est la valeur associé à la
variable “Longueur Aileron (mm)”. Cette valeur correspond à l’incrément de la
masse corporelle lorsqu’un pingouin à un incrément de 1 mm de longueur
d’aileron. Dans notre cas, cette valeur est d’environ 50 grammes. La valeur
de la variable <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> correspond à la valeur de l’ordonnée à l’origine
: un pinguoin avec un aileron de 0 mm aura une masse corporelle de -5781
grammes! Cette valeur est beaucoup plus compliquée à comprendre mais elle
nous permet d’avoir un modèle plus flexible, ne passant pas par l’origine.</p>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1>%%<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>_ = data.plot.scatter(x=”Flipper Length (mm)”, y=”Body Mass (g)”)</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="markdown">
<h1>%% [markdown]<a class="headerlink" href="#markdown" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id2">
<h1><a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="we-observe-that-there-is-a-reasonable-linear-relationship-between-the-flipper">
<h1>We observe that there is a reasonable linear relationship between the flipper<a class="headerlink" href="#we-observe-that-there-is-a-reasonable-linear-relationship-between-the-flipper" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="length-and-the-body-mass-here-our-target-to-be-predicted-will-be-the-body">
<h1>length and the body mass. Here, our target to be predicted will be the body<a class="headerlink" href="#length-and-the-body-mass-here-our-target-to-be-predicted-will-be-the-body" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="mass-while-the-flipper-length-will-be-a-feature">
<h1>mass while the flipper length will be a feature.<a class="headerlink" href="#mass-while-the-flipper-length-will-be-a-feature" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id3">
<h1>%%<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h1>
<p>X, y = data[[“Flipper Length (mm)”]], data[“Body Mass (g)”]</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id4">
<h1>%% [markdown]<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id5">
<h1><a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="in-the-previous-notebook-we-used-a-tt-linearregression-tt-from-scikit-learn-and">
<h1>In the previous notebook, we used a <tt>LinearRegression</tt> from scikit-learn and<a class="headerlink" href="#in-the-previous-notebook-we-used-a-tt-linearregression-tt-from-scikit-learn-and" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="show-that-we-could-learn-the-state-of-the-model-from-the-data-when-calling">
<h1>show that we could learn the state of the model from the data when calling<a class="headerlink" href="#show-that-we-could-learn-the-state-of-the-model-from-the-data-when-calling" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="tt-fit-tt-and-use-these-states-for-prediction-when-calling-the-method">
<h1><tt>fit</tt> and use these states for prediction when calling the method<a class="headerlink" href="#tt-fit-tt-and-use-these-states-for-prediction-when-calling-the-method" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="tt-predict-tt">
<h1><tt>predict</tt>.<a class="headerlink" href="#tt-predict-tt" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id6">
<h1>%%<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h1>
<p>from sklearn.linear_model import LinearRegression</p>
<p>model = LinearRegression()
model.fit(X, y)
y_pred = model.predict(X)</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id7">
<h1>%%<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h1>
<p>ax = data.plot.scatter(x=”Flipper Length (mm)”, y=”Body Mass (g)”)
ax.plot(X, y_pred, label=model.<strong>class</strong>.<strong>name</strong>, color=”black”, linewidth=4)
_ = ax.legend()</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id8">
<h1>%% [markdown]<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id9">
<h1><a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="in-the-previous-notebook-we-quickly-mentioned-that-the-linear-regression-model-was">
<h1>In the previous notebook, we quickly mentioned that the linear regression model was<a class="headerlink" href="#in-the-previous-notebook-we-quickly-mentioned-that-the-linear-regression-model-was" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="minizing-an-error-between-the-true-target-and-the-predicted-target-this-error-is-also">
<h1>minizing an error between the true target and the predicted target. This error is also<a class="headerlink" href="#minizing-an-error-between-the-true-target-and-the-predicted-target-this-error-is-also" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="known-as-loss-function-the-loss-that-is-minimized-in-this-case-is-known-as-the-least">
<h1>known as loss function. The loss that is minimized in this case is known as the least<a class="headerlink" href="#known-as-loss-function-the-loss-that-is-minimized-in-this-case-is-known-as-the-least" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="squared-error-this-loss-is-defined-as">
<h1>squared error. This loss is defined as:<a class="headerlink" href="#squared-error-this-loss-is-defined-as" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id10">
<h1><a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id11">
<h1>$$<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="loss-y-hat-y-2">
<h1>loss = (y - \hat{y})^2<a class="headerlink" href="#loss-y-hat-y-2" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id12">
<h1>$$<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id13">
<h1><a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="that-is">
<h1>that is<a class="headerlink" href="#that-is" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id14">
<h1><a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id15">
<h1>$$<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="loss-y-x-beta-2">
<h1>loss = (y - X \beta)^2<a class="headerlink" href="#loss-y-x-beta-2" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id16">
<h1>$$<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id17">
<h1><a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="we-can-check-what-the-loss-look-likes-in-practice">
<h1>We can check what the loss look likes in practice:<a class="headerlink" href="#we-can-check-what-the-loss-look-likes-in-practice" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id18">
<h1>%%<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h1>
<p>def se_loss(y_true, y_pred):
loss = (y_true - y_pred) ** 2
return loss</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id19">
<h1>%%<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h1>
<p>import numpy as np</p>
<p>xmin, xmax = -2, 2
xx = np.linspace(xmin, xmax, 100)</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id20">
<h1>%%<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h1>
<p>import matplotlib.pyplot as plt</p>
<p>plt.plot(xx, se_loss(0, xx), label=”SE loss”)
_ = plt.legend()</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id21">
<h1>%% [markdown]<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id22">
<h1><a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="looking-at-the-shape-of-the-loss-function-we-see-that-the-bell-shape-of-the-loss-will">
<h1>Looking at the shape of the loss function, we see that the bell shape of the loss will<a class="headerlink" href="#looking-at-the-shape-of-the-loss-function-we-see-that-the-bell-shape-of-the-loss-will" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="impact-greatly-the-large-error-in-practice-this-will-have-an-impact-on-the-fit">
<h1>impact greatly the large error. In practice, this will have an impact on the fit.<a class="headerlink" href="#impact-greatly-the-large-error-in-practice-this-will-have-an-impact-on-the-fit" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id23">
<h1>%%<a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h1>
<p>data = data.append(
{“Flipper Length (mm)”: 230, “Body Mass (g)”: 300},
ignore_index=True
)
data</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id24">
<h1>%%<a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h1>
<p>_ = data.plot.scatter(x=”Flipper Length (mm)”, y=”Body Mass (g)”)</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id25">
<h1>%%<a class="headerlink" href="#id25" title="Permalink to this headline">¶</a></h1>
<p>X, y = data[[“Flipper Length (mm)”]], data[“Body Mass (g)”]</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id26">
<h1>%%<a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h1>
<p>import numpy as np
sample_weight = np.ones_like(y)
sample_weight[-1] = 10</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id27">
<h1>%%<a class="headerlink" href="#id27" title="Permalink to this headline">¶</a></h1>
<p>model.fit(X, y, sample_weight=sample_weight)
y_pred = model.predict(X)</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id28">
<h1>%%<a class="headerlink" href="#id28" title="Permalink to this headline">¶</a></h1>
<p>ax = data.plot.scatter(x=”Flipper Length (mm)”, y=”Body Mass (g)”)
ax.plot(X, y_pred, label=model.<strong>class</strong>.<strong>name</strong>, color=”black”, linewidth=4)
_ = ax.legend()</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id29">
<h1>%% [markdown]<a class="headerlink" href="#id29" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id30">
<h1><a class="headerlink" href="#id30" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="instead-of-using-the-squared-loss-we-will-use-a-loss-known-as-the-huber-loss-in-this">
<h1>Instead of using the squared loss, we will use a loss known as the Huber loss. In this<a class="headerlink" href="#instead-of-using-the-squared-loss-we-will-use-a-loss-known-as-the-huber-loss-in-this" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="regard-we-will-use-the-huberregressor-model-available-in-scikit-learn-we-will-fit">
<h1>regard, we will use the HuberRegressor model available in scikit-learn. We will fit<a class="headerlink" href="#regard-we-will-use-the-huberregressor-model-available-in-scikit-learn-we-will-fit" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="this-model-in-the-exact-similar-way-that-we-previously-did">
<h1>this model in the exact similar way that we previously did.<a class="headerlink" href="#this-model-in-the-exact-similar-way-that-we-previously-did" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id31">
<h1>%%<a class="headerlink" href="#id31" title="Permalink to this headline">¶</a></h1>
<p>from sklearn.linear_model import HuberRegressor</p>
<p>model = HuberRegressor()
model.fit(X, y, sample_weight=sample_weight)
y_pred = model.predict(X)</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id32">
<h1>%%<a class="headerlink" href="#id32" title="Permalink to this headline">¶</a></h1>
<p>ax = data.plot.scatter(x=”Flipper Length (mm)”, y=”Body Mass (g)”)
ax.plot(X, y_pred, label=model.<strong>class</strong>.<strong>name</strong>, color=”black”, linewidth=4)
_ = ax.legend()</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id33">
<h1>%% [markdown]<a class="headerlink" href="#id33" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id34">
<h1><a class="headerlink" href="#id34" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="we-observe-that-the-outlier-has-much-less-weight-than-in-the-case-of-the-least-squared">
<h1>We observe that the outlier has much less weight than in the case of the least squared<a class="headerlink" href="#we-observe-that-the-outlier-has-much-less-weight-than-in-the-case-of-the-least-squared" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="loss">
<h1>loss.<a class="headerlink" href="#loss" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id35">
<h1>%%<a class="headerlink" href="#id35" title="Permalink to this headline">¶</a></h1>
<p>def huber_loss(y_true, y_pred, *, epsilon):
mask_greater_epsilon = np.abs(y_true - y_pred) &gt; epsilon
loss = np.zeros_like(y_pred)
loss[mask_greater_epsilon] = np.abs(y_true - y_pred)[mask_greater_epsilon]
loss[~mask_greater_epsilon] = se_loss(y_true, y_pred)[~mask_greater_epsilon]
return loss</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id36">
<h1>%%<a class="headerlink" href="#id36" title="Permalink to this headline">¶</a></h1>
<p>def absolute_loss(y_true, y_pred):
loss = np.abs(y_true - y_pred)
return loss</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id37">
<h1>%%<a class="headerlink" href="#id37" title="Permalink to this headline">¶</a></h1>
<p>plt.plot(xx, se_loss(0, xx), label=”SE loss”)
plt.plot(xx, huber_loss(0, xx, epsilon=1), label=”Huber loss”)
plt.plot(xx, absolute_loss(0, xx), label=”Absolute loss”, linestyle=”–”)
plt.ylabel(“Loss”)
plt.xlabel(“xx”)
_ = plt.legend(loc=”center left”, bbox_to_anchor=(1, 0.5))</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id38">
<h1>%% [markdown]<a class="headerlink" href="#id38" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id39">
<h1><a class="headerlink" href="#id39" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="we-observe-that-the-huber-and-absolute-losses-are-penalizing-less-outliers-it-means">
<h1>We observe that the Huber and absolute losses are penalizing less outliers. It means<a class="headerlink" href="#we-observe-that-the-huber-and-absolute-losses-are-penalizing-less-outliers-it-means" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="that-these-outliers-will-be-less-attractive-and-we-will-not-try-to-find-beta-that">
<h1>that these outliers will be less attractive and we will not try to find <span class="math notranslate nohighlight">\(\beta\)</span> that<a class="headerlink" href="#that-these-outliers-will-be-less-attractive-and-we-will-not-try-to-find-beta-that" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="try-to-minimize-this-large-error-indeed-the-tt-huberregressor-tt-will-give-an">
<h1>try to minimize this large error. Indeed, the <tt>HuberRegressor</tt> will give an<a class="headerlink" href="#try-to-minimize-this-large-error-indeed-the-tt-huberregressor-tt-will-give-an" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="estimator-of-the-median-instead-of-the-mean">
<h1>estimator of the median instead of the mean.<a class="headerlink" href="#estimator-of-the-median-instead-of-the-mean" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id40">
<h1><a class="headerlink" href="#id40" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="if-one-is-interesting-in-other-quantile-than-the-median-scikit-learn-provides-an">
<h1>If one is interesting in other quantile than the median, scikit-learn provides an<a class="headerlink" href="#if-one-is-interesting-in-other-quantile-than-the-median-scikit-learn-provides-an" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="estimator-called-quantileregressor-that-minimizes-the-pinball-loss-and-provide-a">
<h1>estimator called <code class="docutils literal notranslate"><span class="pre">QuantileRegressor</span></code> that minimizes the pinball loss and provide a<a class="headerlink" href="#estimator-called-quantileregressor-that-minimizes-the-pinball-loss-and-provide-a" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="estimator-of-the-requested-quantile-for-instance-one-could-request-the-median-in-the">
<h1>estimator of the requested quantile. For instance, one could request the median in the<a class="headerlink" href="#estimator-of-the-requested-quantile-for-instance-one-could-request-the-median-in-the" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="following-manner">
<h1>following manner:<a class="headerlink" href="#following-manner" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id41">
<h1>%%<a class="headerlink" href="#id41" title="Permalink to this headline">¶</a></h1>
<p>from sklearn.linear_model import QuantileRegressor</p>
<p>model = QuantileRegressor(quantile=0.5)
model.fit(X, y, sample_weight=sample_weight)
y_pred = model.predict(X)</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id42">
<h1>%%<a class="headerlink" href="#id42" title="Permalink to this headline">¶</a></h1>
<p>ax = data.plot.scatter(x=”Flipper Length (mm)”, y=”Body Mass (g)”)
ax.plot(X, y_pred, label=model.<strong>class</strong>.<strong>name</strong>, color=”black”, linewidth=4)
_ = ax.legend()</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id43">
<h1>%% [markdown]<a class="headerlink" href="#id43" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id44">
<h1><a class="headerlink" href="#id44" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="principe-de-la-regularisation">
<h1>### Principe de la régularisation<a class="headerlink" href="#principe-de-la-regularisation" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id45">
<h1><a class="headerlink" href="#id45" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="in-this-first-example-we-will-show-a-known-issue-due-to-correlated-features-when">
<h1>In this first example, we will show a known issue due to correlated features when<a class="headerlink" href="#in-this-first-example-we-will-show-a-known-issue-due-to-correlated-features-when" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="fitting-a-linear-model">
<h1>fitting a linear model.<a class="headerlink" href="#fitting-a-linear-model" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id46">
<h1><a class="headerlink" href="#id46" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="the-data-generative-process-to-create-the-data-is-a-linear-relationship-between-the">
<h1>The data generative process to create the data is a linear relationship between the<a class="headerlink" href="#the-data-generative-process-to-create-the-data-is-a-linear-relationship-between-the" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="features-and-the-target-however-out-of-5-features-only-2-features-will-be-used">
<h1>features and the target. However, out of 5 features, only 2 features will be used<a class="headerlink" href="#features-and-the-target-however-out-of-5-features-only-2-features-will-be-used" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="while-3-other-features-will-not-be-linked-to-the-target-in-addition-a-little-bit-of">
<h1>while 3 other features will not be linked to the target. In addition, a little bit of<a class="headerlink" href="#while-3-other-features-will-not-be-linked-to-the-target-in-addition-a-little-bit-of" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="noise-will-be-added-when-generating-the-dataset-we-can-as-well-get-the-true-model">
<h1>noise will be added. When generating the dataset, we can as well get the true model.<a class="headerlink" href="#noise-will-be-added-when-generating-the-dataset-we-can-as-well-get-the-true-model" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id47">
<h1>%%<a class="headerlink" href="#id47" title="Permalink to this headline">¶</a></h1>
<p>from sklearn.datasets import make_regression</p>
<p>data, target, coef = make_regression(
n_samples=2_000,
n_features=5,
n_informative=2,
shuffle=False,
coef=True,
random_state=0,
noise=30,
)</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id48">
<h1>%%<a class="headerlink" href="#id48" title="Permalink to this headline">¶</a></h1>
<p>import seaborn as sns
sns.set_context(“poster”)</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id49">
<h1>%%<a class="headerlink" href="#id49" title="Permalink to this headline">¶</a></h1>
<p>import pandas as pd</p>
<p>feature_names = [f”Features {i}” for i in range(data.shape[1])]
coef = pd.Series(coef, index=feature_names)
coef.plot.barh()
coef</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id50">
<h1>%% [markdown]<a class="headerlink" href="#id50" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id51">
<h1><a class="headerlink" href="#id51" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="plotting-the-true-coefficients-we-observe-that-only-2-features-out-of-the-5-features">
<h1>Plotting the true coefficients, we observe that only 2 features out of the 5 features<a class="headerlink" href="#plotting-the-true-coefficients-we-observe-that-only-2-features-out-of-the-5-features" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="as-an-impact-on-the-target">
<h1>as an impact on the target.<a class="headerlink" href="#as-an-impact-on-the-target" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id52">
<h1><a class="headerlink" href="#id52" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="now-we-will-fit-a-linear-model-on-this-dataset">
<h1>Now, we will fit a linear model on this dataset.<a class="headerlink" href="#now-we-will-fit-a-linear-model-on-this-dataset" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id53">
<h1>%%<a class="headerlink" href="#id53" title="Permalink to this headline">¶</a></h1>
<p>from sklearn.linear_model import LinearRegression</p>
<p>linear_regression = LinearRegression()
linear_regression.fit(data, target)
linear_regression.coef_</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id54">
<h1>%%<a class="headerlink" href="#id54" title="Permalink to this headline">¶</a></h1>
<p>feature_names = [f”Features {i}” for i in range(data.shape[1])]
coef = pd.Series(linear_regression.coef_, index=feature_names)
_ = coef.plot.barh()</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id55">
<h1>%% [markdown]<a class="headerlink" href="#id55" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id56">
<h1><a class="headerlink" href="#id56" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="we-observe-that-we-can-recover-almost-the-true-coefficients-the-small-fluctuation-are">
<h1>We observe that we can recover almost the true coefficients. The small fluctuation are<a class="headerlink" href="#we-observe-that-we-can-recover-almost-the-true-coefficients-the-small-fluctuation-are" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="due-to-the-noise-that-we-added-into-the-dataset-when-generating-it">
<h1>due to the noise that we added into the dataset when generating it.<a class="headerlink" href="#due-to-the-noise-that-we-added-into-the-dataset-when-generating-it" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id57">
<h1>%%<a class="headerlink" href="#id57" title="Permalink to this headline">¶</a></h1>
<p>import numpy as np</p>
<p>data = np.concatenate([data, data[:, [0, 1]], data[:, [0, 1]]], axis=1)</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id58">
<h1>%%<a class="headerlink" href="#id58" title="Permalink to this headline">¶</a></h1>
<p>linear_regression = LinearRegression()
linear_regression.fit(data, target)
linear_regression.coef_</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id59">
<h1>%%<a class="headerlink" href="#id59" title="Permalink to this headline">¶</a></h1>
<p>feature_names = [f”Features {i}” for i in range(data.shape[1])]
coef = pd.Series(linear_regression.coef_, index=feature_names)
_ = coef.plot.barh()</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id60">
<h1>%% [markdown]<a class="headerlink" href="#id60" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id61">
<h1><a class="headerlink" href="#id61" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id62">
<h1>%% [markdown]<a class="headerlink" href="#id62" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id63">
<h1><a class="headerlink" href="#id63" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="importance-du-pretraitement">
<h1>### Importance du prétraitement<a class="headerlink" href="#importance-du-pretraitement" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id64">
<h1><a class="headerlink" href="#id64" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="alternative-a-la-minimization-moindres-carres">
<h1>### Alternative à la minimization moindres carrés<a class="headerlink" href="#alternative-a-la-minimization-moindres-carres" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id65">
<h1><a class="headerlink" href="#id65" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="resolution-de-problemes-non-lineaires">
<h1>## Résolution de problèmes non-linéaires<a class="headerlink" href="#resolution-de-problemes-non-lineaires" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="id66">
<h1><a class="headerlink" href="#id66" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="quantifier-l-incertitude-des-predictions">
<h1>## Quantifier l’incertitude des prédictions<a class="headerlink" href="#quantifier-l-incertitude-des-predictions" title="Permalink to this headline">¶</a></h1>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapter_00"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="file_00.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Introduction</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="file_02.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Famille des modèles non-paramétriques</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Cédric Lemaître<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>