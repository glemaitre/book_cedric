
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Famille des modèles paramétriques &#8212; Cédric&#39;s book</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Famille des modèles non-paramétriques" href="file_02.html" />
    <link rel="prev" title="Introduction" href="file_00.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Cédric's book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../landing-page.html">
   Table of content
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="chapter_00_intro.html">
   Apprentisage automatique supervisé
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="file_00.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Famille des modèles paramétriques
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="file_02.html">
     Famille des modèles non-paramétriques
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="file_03.html">
     Recherche des hyperparamètres
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="file_04.html">
     Gestion des variables catégorielles
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter_99/chapter_99_intro.html">
   Real-world examples
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter_99/file_00.html">
     Analyse de statistique tradionnelle
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/chapter_00/file_01.py"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.py</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modele-lineaire">
   Modèle linéaire
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#trouver-le-meilleur-modele-possible">
     Trouver le meilleur modèle possible
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#alternative-a-la-methode-des-moindres-carres">
     Alternative à la méthode des moindres carrés
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principe-de-la-regularisation">
     Principe de la régularisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#importance-du-pretraitement">
     Importance du prétraitement
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resolution-de-problemes-non-lineaires">
   Résolution de problèmes non-linéaires
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quantifier-l-incertitude-des-predictions">
   Quantifier l’incertitude des prédictions
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="famille-des-modeles-parametriques">
<h1>Famille des modèles paramétriques<a class="headerlink" href="#famille-des-modeles-parametriques" title="Permalink to this headline">¶</a></h1>
<p>Dans le chapitre précédent, nous avons détaillé le principe d’un modèle
prédictif de manière mathématique. Nous pouvons rappeler cette formulation :</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f(X)
\]</div>
<p>Nous avons même donné un exemple d’un modèle assez naif ou nous avons utilisé
une relation entre notre variable d’entrée et notre variable de sortie. Cette
manière de définir un modèle est nommée <strong>modèle paramétrique</strong>. En effet,
nous avons défini un modèle paramétrique de la forme suivante :</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f(X) = X \beta
\]</div>
<p>Le paramètre <span class="math notranslate nohighlight">\(\beta\)</span> est donc le <strong>paramètre</strong> de notre modèle. L’idée
derrière cette famille de modèles est donc de pouvoir compresser
l’information de notre jeu de données d’apprentissage avec seulement quelques
paramètres et un <em>apriori</em> sur la relation entre <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(y\)</span> (nous reviendrons
plus en détail sur cet aspect dans les sections qui viennent).</p>
<p>Dans ce chapitre, nous allons tout d’abord détailler une des familles les
plus simple : les modèles linéaires. Nous présenterons certaines composantes
importantes de ce type de modèle. Par la suite, nous montrerons que ces
modèles peuvent également utilisés pour des problèmes non-linéaires.</p>
<div class="section" id="modele-lineaire">
<h2>Modèle linéaire<a class="headerlink" href="#modele-lineaire" title="Permalink to this headline">¶</a></h2>
<p>Un modèle linéaire est un modèle paramétrique qui est défini par une relation
entre <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(y\)</span> tel que <span class="math notranslate nohighlight">\(y\)</span> est une combination linéaire de <span class="math notranslate nohighlight">\(X\)</span>. Notre
modèle dans le chapitre précédent était un modèle linéaire :</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f(X, \beta) = X \beta
\]</div>
<p>Le terme “combination” nous indique que nous pouvons généraliser cette
relation en combinant toutes les variables d’entrée (i.e. colonnes) de <span class="math notranslate nohighlight">\(X\)</span>.
Un tel modèle est donc défini de la manière suivante :</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f(X, \beta) = X \beta = \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n
X_n
\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta_n\)</span> est le paramètre associé à la variable <span class="math notranslate nohighlight">\(X_n\)</span>. La relation
ci-dessus force notre modèle de prédire 0 lorsque les valeurs dans <span class="math notranslate nohighlight">\(X\)</span> sont
également à 0. Pour avoir plus de flexibilité, un paramètre <span class="math notranslate nohighlight">\(\beta_0\)</span> est
utilisé pour représenter cette constante et est appelé l’<strong>intercept</strong>.</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f(X, \beta) = X \beta = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... +
\beta_n X_n
\]</div>
<p>Nous pouvons donc comprendre que nous faisons un <em>apriori</em> entre le lien
entre <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(y\)</span> : nous pensons que <span class="math notranslate nohighlight">\(y\)</span> est une combinaison linéaire de <span class="math notranslate nohighlight">\(X\)</span>
et que cet relations est suffisante. Un peu plus tard dans ce chapitre, nous
donnerons des exemples où ce n’est pas le cas et où nous devrons modifier la
formulation de notre modèle.</p>
<p>En revanche, si cet <em>apriori</em> est correct, nous venons donc de compresser
notre de dataset de taille composé de <span class="math notranslate nohighlight">\(N\)</span> échantillons à un modèle de taille
<span class="math notranslate nohighlight">\(P + 1\)</span> paramètres (i.e. + 1 corresponds à l’intercept). Maintenant que nous
avons défini notre modèle, il nous est possible de trouver les paramètres.</p>
<div class="section" id="trouver-le-meilleur-modele-possible">
<h3>Trouver le meilleur modèle possible<a class="headerlink" href="#trouver-le-meilleur-modele-possible" title="Permalink to this headline">¶</a></h3>
<p>Maintenant que nous connaisons la paramétrisation de notre modèle, nous
pouvons l’illustrer sur le même jeu de données que nous avons utilisé dans
le chapitre précédent. Tout d’abord, nous chargeons les données.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">donnees</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../datasets/penguins_regression.csv&quot;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">donnees</span><span class="p">[[</span><span class="s2">&quot;Longueur Aileron (mm)&quot;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">donnees</span><span class="p">[</span><span class="s2">&quot;Masse Corporelle (g)&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Et nous pouvons visualaliser la relation entre <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(y\)</span> :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;poster&quot;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Masse corporelle en fonction de</span><span class="se">\n</span><span class="s2">la longueur d&#39;aileron&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_3_0.png" src="../_images/file_01_3_0.png" />
</div>
</div>
<p>Il existe une infinité de modèle linéaire qui pourraient être utilisés pour
pour prédire la masse corporelle de nos pingouins. Définissons une fonction
Python générique qui permet de prédire la masse corporelle de notre pinguoin
en fonction de la longueur d’aileron.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">modele_lineaire</span><span class="p">(</span><span class="n">longueur_aileron</span><span class="p">,</span> <span class="n">parametres</span><span class="p">):</span>
    <span class="c1"># notre modèle est défini par: y = beta_0 + x_1 * beta_1</span>
    <span class="k">return</span> <span class="n">parametres</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">parametres</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">longueur_aileron</span>
</pre></div>
</div>
</div>
</div>
<p>Maintenant que nous avons notre modèle, nous pouvons visualiser quelques
modèles avec différents paramètres.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions_modele_1</span> <span class="o">=</span> <span class="n">modele_lineaire</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">3_000</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
<span class="n">predictions_modele_2</span> <span class="o">=</span> <span class="n">modele_lineaire</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">6_000</span><span class="p">,</span> <span class="mi">50</span><span class="p">])</span>
<span class="n">predictions_modele_3</span> <span class="o">=</span> <span class="n">modele_lineaire</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">2_000</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_modele_1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Modèle #1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_modele_2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Modèle #2&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_modele_3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Modèle #3&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Quel modèle est le meilleur?&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_8_0.png" src="../_images/file_01_8_0.png" />
</div>
</div>
<p>A partir de ce graphique, la question que nous pourrions avoir est de savoir
quel modèle est le meilleur. Qualitativement, nous pourrions dire que le
modèle #1 est le pire modèle. Entre le modèle #2 et #3, nous pourrions
préviligier le modèle #2 car il semble plus “centré” avec nos données.</p>
<p>Cependant, choisir un modèle ne peut-être basé sur une évaluation
qualitative. Dans le chapitre précédent, nous avons utilisé différentes
méthodes qui calculaient une erreur. Nous pouvons ici calculer une erreur
donnée : l’erreur quadratique moyenne.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Erreur du modèle #1: </span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_modele_1</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Erreur du modèle #2: </span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_modele_2</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Erreur du modèle #3: </span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_modele_3</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Erreur du modèle #1: 1609923.83
Erreur du modèle #2: 178899.85
Erreur du modèle #3: 261327.34
</pre></div>
</div>
</div>
</div>
<p>En utilisant cette erreur, nous avons la confirmation que le modèle #2 a la
plus petite erreur. En revanche, est ce que ce modèle est le meilleur
possible ? Si non, comment pouvons nous trouver un modèle la plus faible
possible ?</p>
<p>Nous donc un probème d’optimisation où nous voudrions minimiser cette erreur
également appelée <strong>fonction de coût</strong> dans ce contexte. Donc, nous pouvons
donc définir notre fonctionde coup comme l’erreur quadratique moyenne
formulée ci-dessous :</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\beta) = \frac{1}{N} \sum_{i=1}^N \left( y_i - f(X_i, \beta)
\right)^2
\]</div>
<p>Et nous chercherons donc à minimiser cette fonction de coût. En d’autre
termes, nous serions intéressés par trouver le minimum de
<span class="math notranslate nohighlight">\(\mathcal{L}(\beta)\)</span>. Cette minimisation est également appellée méthode
des moindres carrés.</p>
<div class="math notranslate nohighlight">
\[
\min_{\beta} \mathcal{L}(\beta)
\]</div>
<p>Trouver le minimum d’une fonction donnée est un problème typique
d’<strong>optimisation methématique</strong> et il existe plusieurs méthodes, certaines
plus performantes que d’autres, dépendant de la fonction à minimiser. Nous
pouvons mentionner les méthodes basées sur le gradient qui nécessitent de
pouvoir dériver la fonction de coût.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Il existe une solution analytique pour la fonction de coût que nous avons
définie.</p>
<div class="math notranslate nohighlight">
\[
\beta = \left( X^T X \right)^{-1} X^T y
\]</div>
<p>En revanche, nous avons introduit la méthode de gradient car elle nous
permettra d’avoir une certaine réflexion concernant les futurs fonctions de
coût que nous allons définir.</p>
</div>
<p>Nous avons la chance que notre fonction de coût définie comme l’erreur
quadratique moyenne soit facilement dérivable. Il serait donc facile de
calculer le mimimum de cette fonction de coût.</p>
<p><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> nous propose une classe dénommée <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> qui
permet de minimser cette fonction de coût. Nous allons la mettre en pratique
dès maintenant.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">modele</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">modele</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearRegression()
</pre></div>
</div>
</div>
</div>
<p>Maintenant que notre modèle est entrainé, nous pouvons l’utiliser pour
observer visuellement quels sont les prédictions produites par ce modèle.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">modele</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Regression linéaire&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Modele LinearRegression&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_15_0.png" src="../_images/file_01_15_0.png" />
</div>
</div>
<p>Ce modèle semble donc bien minimiser la fonction de coût et est
qualititivement correct. Nous pouvons donc maintenant nous pencher sur notre
modèle et obtenir la valeurs des paramètres.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modele</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([49.68556641])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modele</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-5780.831358077066
</pre></div>
</div>
</div>
</div>
<p>Nous pouvons donc observer deux attributs qui correspondent aux paramètres de
notre modèle. <code class="docutils literal notranslate"><span class="pre">coef_</span></code> contient les paramètres de <span class="math notranslate nohighlight">\(\beta_1, ..., \beta_n\)</span>
alors que <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> contient le paramètre de <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Dans scikit-learn, les attributs finissant par <code class="docutils literal notranslate"><span class="pre">_</span></code> sont des attributs
qui sont créés après avoir appelé la méthode <code class="docutils literal notranslate"><span class="pre">fit()</span></code>. Ils sont liés à
l’algorithme d’apprentissage et seront nécessaires pour pouvoir créer
des prédictions.</p>
</div>
<p>Maintenant, nous pouvons interpréter la valeurs des paramètres de notre
modèle. La valeur dans la variable <code class="docutils literal notranslate"><span class="pre">coef_</span></code> est la valeur associé à la
variable “Longueur Aileron (mm)”. Cette valeur correspond à l’incrément de la
masse corporelle lorsqu’un pingouin à un incrément de 1 mm de longueur
d’aileron. Dans notre cas, cette valeur est d’environ 50 grammes. La valeur
de la variable <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> correspond à la valeur de l’ordonnée à l’origine
: un pinguoin avec un aileron de 0 mm aura une masse corporelle de -5781
grammes! Cette valeur est beaucoup plus compliquée à comprendre mais elle
nous permet d’avoir un modèle plus flexible, ne passant pas par l’origine.</p>
</div>
<div class="section" id="alternative-a-la-methode-des-moindres-carres">
<h3>Alternative à la méthode des moindres carrés<a class="headerlink" href="#alternative-a-la-methode-des-moindres-carres" title="Permalink to this headline">¶</a></h3>
<p>La méthode des moindres carrés est la méthode la plus simple que nou pouvions
présenter. En revanche, cette méthode à des limitations connues. Nous allons
en présenter l’une d’entre elles, ainsi montrer comment nous pouvons la
contourner.</p>
<p>Nous allons reprendre les mêmes données que précédemment utilisées. En
revanche, nous allons simuler que des erreurs de saisie sont survenues lors
de la collecte des données. Pour cela, nous allons rajouter un échantillon de
pinguoin qui aura un aileron de longueur de 230 mm et une masse corporelle de
300 grammes. Cette erreur pourrait être dûe à une erreur de saisie ou un 0
est manquant.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">donnees</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;Longueur Aileron (mm)&quot;</span><span class="p">:</span> <span class="mi">230</span><span class="p">,</span> <span class="s2">&quot;Masse Corporelle (g)&quot;</span><span class="p">:</span> <span class="mi">300</span><span class="p">},</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">donnees</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">donnees</span><span class="p">[[</span><span class="s2">&quot;Longueur Aileron (mm)&quot;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">donnees</span><span class="p">[</span><span class="s2">&quot;Masse Corporelle (g)&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Nos données avec une</span><span class="se">\n</span><span class="s2">erreur de saisie&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_21_0.png" src="../_images/file_01_21_0.png" />
</div>
</div>
<p>Nous pouvons observer que nous avons un nouveau échantillon dans le cadrant
en bas à droite de notre graphique. Nous allons maintenant entraîner un
modèle linéaire qui minimise l’erreur quadratique moyenne. Pour simuler que
nous avons plusieurs erreurs de saisie, nous allons entraîner notre modèle en
assignant des poids à chaque échantillon : tous les échantillons auront un
poids de 1, sauf le nouvel échantillon qui aura un poids de 10.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">poids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
<span class="n">poids</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modele</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">predictions_err_quadratique</span> <span class="o">=</span> <span class="n">modele</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">poids</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_err_quadratique</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Regression linéaire&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Modele LinearRegression&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_25_0.png" src="../_images/file_01_25_0.png" />
</div>
</div>
<p>Nous pouvons donc observer que le faite d’avoir des erreurs de saisie à une
influence non négligeable sur la qualité de notre modèle. Ceci peut-être
expliqué par le type de fonction de coût que nous utilisons. Il sera plus
facile d’obtenir une intuition en représentant graphiquement cette fonction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">erreur_quadratique</span><span class="p">(</span><span class="n">cible</span><span class="p">,</span> <span class="n">prediction</span><span class="p">):</span>
    <span class="n">cout</span> <span class="o">=</span> <span class="p">(</span><span class="n">cible</span> <span class="o">-</span> <span class="n">prediction</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">cout</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span> <span class="o">=</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">erreur_quadratique</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xx</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Erreur quadratique&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_28_0.png" src="../_images/file_01_28_0.png" />
</div>
</div>
<p>Nous pouvons donc observer que le faite d’élever au carré l’erreur pénalise
extrêment les échantillons avec une grande erreur. Il serait donc intéressant
d’utiliser une fonction de coût qui affectera un coût moindre aux
échantillons pour lesquel notre modèle commet le plus d’erreur. Au lieu de
prendre le carré de l’erreur, nous pourrions seulement utiliser la valeur
absolue de l’erreur. Cette erreur sera donc l’<strong>erreur absolute moyenne</strong> et
peut-être formulée comme suit :</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\beta) = \frac{1}{N} \sum_{i=1}^N \left| y_i - f(X_i, \beta)
\right|
\]</div>
<p>Nous pouvons comparer visualement la représentation de cette fonction de coût
avec la représentation de la fonction d’erreur quadratique.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">erreur_absolue</span><span class="p">(</span><span class="n">cible</span><span class="p">,</span> <span class="n">prediction</span><span class="p">):</span>
    <span class="n">cout</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">cible</span> <span class="o">-</span> <span class="n">prediction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cout</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">erreur_quadratique</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xx</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Erreur quadratique&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">erreur_absolue</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xx</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Erreur absolue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Comparaison des fonctions de coût&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_31_0.png" src="../_images/file_01_31_0.png" />
</div>
</div>
<p>On peut donc observer que l’erreur absolue pénalisera les échantillons avec
une grande erreur. En revanche, si nous nous attardons sur l’erreur aboslue
nous pouvons observer quelle n’est pas dérivable en 0. Ceci nous empêche
d’utiliser une méthode d’optimisation basée sur le gradient ce qui est
problématique.</p>
<p>Si nous voulons utiliser une méthode par descente de gradient, il est donc
nécessaire de trouver un moyen de combiner les deux fonctions de coût :
utiliser l’erreur absolue loin de 0 pour moins pénaliser les échantillons
avec une grande erreur et utiliser l’erreur quadratique quand l’erreur est
proche de 0 pour que nous puissions déterminer le gradient.</p>
<p>Cette fonction de coût est connu sous le nom de la fonction de Huber et est
formulée comme suit :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{L}(\beta) = \frac{1}{N} \sum^{N}_{i=1} \begin{cases}
\left( y_i - f(X_i, \beta) \right)^2 &amp; \text{si } \left|
y_i - f(X_i, \beta) \right| &lt; \epsilon \\
2 \epsilon \left( | y_i - f(X_i, \beta) | - \epsilon^2 \right) &amp; \text{sinon}
\end{cases}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fonction_huber</span><span class="p">(</span><span class="n">cible</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.35</span><span class="p">):</span>
    <span class="n">cout_absolue</span> <span class="o">=</span> <span class="n">erreur_absolue</span><span class="p">(</span><span class="n">cible</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
    <span class="n">cout_quadratique</span> <span class="o">=</span> <span class="n">erreur_quadratique</span><span class="p">(</span><span class="n">cible</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>

    <span class="n">plus_grand_epsilon</span> <span class="o">=</span> <span class="n">cout_absolue</span> <span class="o">&gt;</span> <span class="n">epsilon</span>

    <span class="n">cout</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
    <span class="n">cout</span><span class="p">[</span><span class="o">~</span><span class="n">plus_grand_epsilon</span><span class="p">]</span> <span class="o">=</span> <span class="n">cout_quadratique</span><span class="p">[</span><span class="o">~</span><span class="n">plus_grand_epsilon</span><span class="p">]</span>
    <span class="n">cout</span><span class="p">[</span><span class="n">plus_grand_epsilon</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="mi">2</span> <span class="o">*</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">cout_absolue</span><span class="p">[</span><span class="n">plus_grand_epsilon</span><span class="p">]</span> <span class="o">-</span> <span class="n">epsilon</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">cout</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">erreur_quadratique</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xx</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Erreur quadratique&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">erreur_absolue</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xx</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Erreur absolue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">fonction_huber</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xx</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Fonction de Huber&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Comparaison des fonctions de coût&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_34_0.png" src="../_images/file_01_34_0.png" />
</div>
</div>
<p>Nous pouvons donc observer que la fonction de Huber a les avantages des deux
fonctions de coût précédentes. <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> propose une classe appelée
<code class="docutils literal notranslate"><span class="pre">HuberRegressor</span></code> qui permettra d’optimiser cette fonction de coût. Nous
allons donc utiliser ce modèle sur notre jeu de données et observer la
différence sur les prédictions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">HuberRegressor</span>

<span class="n">modele</span> <span class="o">=</span> <span class="n">HuberRegressor</span><span class="p">()</span>
<span class="n">predictions_err_huber</span> <span class="o">=</span> <span class="n">modele</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">poids</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_err_quadratique</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Quadratique&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_err_huber</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Huber&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Comparaison modèle linéaire&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_37_0.png" src="../_images/file_01_37_0.png" />
</div>
</div>
<p>Nous pouvons donc constater que le modèle linéaire minimisant la fonction de
Huber permet d’obtenir un meilleur modèle que celui minimisant la fonction
de coût quadratique.</p>
<p>Pour confirmer de manière quantitative, nous pourrions calculer des erreurs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Modèle linéaire quadratique:</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;  Erreur quadratique moyenne : &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_err_quadratique</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;  Erreur absolue moyenne : &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_err_quadratique</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Modèle linéaire Huber:</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;  Erreur quadratique moyenne : &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_err_huber</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;  Erreur absolue moyenne : &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_err_huber</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Modèle linéaire quadratique:
  Erreur quadratique moyenne : 313313.17
  Erreur absolue moyenne : 412.61
Modèle linéaire Huber:
  Erreur quadratique moyenne : 236451.75
  Erreur absolue moyenne : 328.44
</pre></div>
</div>
</div>
</div>
<p>Nous pouvons confirmer que le modèle linéaire quadratique a une erreur
quadratique moins élevèe que le modèle linéaire Huber. En revanche, nous
avons la conclusion opposée pour l’erreur absolue.</p>
<p>Il est quand même intéressant de mentionner qu’il es possible d’optimiser la
fonction de coût basée sur l’erreur absolue. En revanche, la méthode
d’optimisation sera différente. Ce type de d’estimateur est connue en anglais
sous le nom de “Least Absolute Deviation” (LAD). Cet estimateur minimisera
donc la fonction de coût des erreurs absolues et sera un estimateur de la
médiane de nos données. <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> propose une classe appelée
<code class="docutils literal notranslate"><span class="pre">QuantileRegressor</span></code> qui permet de regresser n’importe quelle quantile et
notemment la médiane.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">QuantileRegressor</span>

<span class="n">modele</span> <span class="o">=</span> <span class="n">QuantileRegressor</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;highs&quot;</span><span class="p">)</span>
<span class="n">predictions_err_absolue</span> <span class="o">=</span> <span class="n">modele</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">poids</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_err_quadratique</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Quadratique&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_err_huber</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Huber&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_err_absolue</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Absolue&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Comparaison modèle linéaire&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_42_0.png" src="../_images/file_01_42_0.png" />
</div>
</div>
<p>Nous pouvons donc constater que l’estimateur basé sur la fonction de coût de
Huber est très proche de l’estimateur de la médiane. Nous reviendrons plus
tard dans ce chapitre sur l’estimateur quantiles pour estimer des intervalles
de confiance autour de la médiane.</p>
</div>
<div class="section" id="principe-de-la-regularisation">
<h3>Principe de la régularisation<a class="headerlink" href="#principe-de-la-regularisation" title="Permalink to this headline">¶</a></h3>
<p>Dans la section précédente, nous avons abordé le principe de la fonction de
coût. Nous avons étudié et analysé différentes types d’erreur. En revanche,
nous n’avons pas abordé les différentes formes de régularisation.</p>
<p>Dans cette section, nous allons dans un premier temps motivé l’utilité de la
régularisation. Dans un deuxième temps, nous allons présenter les différents
types de régularisation.</p>
<p>Pour motiver l’utilité de la régularisation, nous allons générer un jeu de
données simple que nous allons pouvoir facilement interpréter. Nous allons
générer une cible à partir d’une combination linéaire de deux variables. Nous
allons également ajouter trois variables qui ne seront pas prédictives (i.e.
aucun lien n’existera entre la cible <code class="docutils literal notranslate"><span class="pre">y</span></code> et ces variables). Pour rendre le
problème plus réaliste, nous allons ajouter un bruit Gaussien à la cible.</p>
<p>La fonction <code class="docutils literal notranslate"><span class="pre">make_regression</span></code> dans <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> nous permet de générer un
tel jeu de données.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>

<span class="n">n_features</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">true_coef</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span>
    <span class="n">n_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
    <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">coef</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">noise</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>En plus des données <code class="docutils literal notranslate"><span class="pre">X</span></code> et <code class="docutils literal notranslate"><span class="pre">y</span></code>, cette fonction nous fournit également les
coefficients de la combinaison linéaire. Nous pouvons visualiser ces
coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nom_colonnes</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Colonne #</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">)]</span>
<span class="n">true_coef</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">true_coef</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">nom_colonnes</span><span class="p">)</span>
<span class="n">true_coef</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">barh</span><span class="p">()</span>
<span class="n">true_coef</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Colonne #0    44.145196
Colonne #1    63.006149
Colonne #2     0.000000
Colonne #3     0.000000
Colonne #4     0.000000
Colonne #5     0.000000
dtype: float64
</pre></div>
</div>
<img alt="../_images/file_01_47_1.png" src="../_images/file_01_47_1.png" />
</div>
</div>
<p>Nous pouvons également visualiser les liens marginales entre la cible <code class="docutils literal notranslate"><span class="pre">y</span></code> et
chacune des variables composant notre jeu de données <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">donnees</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">nom_colonnes</span><span class="p">)</span>
<span class="n">donnees</span><span class="p">[</span><span class="s2">&quot;Cible&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
<span class="n">donnees</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="s2">&quot;Cible&quot;</span><span class="p">,</span> <span class="n">var_name</span><span class="o">=</span><span class="s2">&quot;Variable&quot;</span><span class="p">,</span> <span class="n">value_name</span><span class="o">=</span><span class="s2">&quot;Valeur&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Valeur&quot;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Cible&quot;</span><span class="p">,</span>
    <span class="n">col</span><span class="o">=</span><span class="s2">&quot;Variable&quot;</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;Variable&quot;</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">donnees</span><span class="p">,</span>
    <span class="n">col_wrap</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">ci</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">scatter_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;s&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.FacetGrid at 0x7f1eb03cd4d0&gt;
</pre></div>
</div>
<img alt="../_images/file_01_50_1.png" src="../_images/file_01_50_1.png" />
</div>
</div>
<p>Sur ce graphique, nous pouvons donc confirmer qu’il existe une lien entre les
deux premières variables et notre cible alors qu’il n’existe aucun lien avec
les autres variables.</p>
<p>Maintenant, que nous avons quelques intuitions concernant notre jeu de
données, nous allons créer un modèle linéaire pour estimer les coefficients
de la combinaison linéaire. Mais tout d’abord, nous allons évaluer un tel
modèle via une validation croisée. Nous utiliserons le score <span class="math notranslate nohighlight">\(R^2\)</span> pour
evaluer notre modèle (ce score est une mesure de la qualité du modèle où le
maximum score est 1).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RepeatedKFold</span>

<span class="n">modele</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">RepeatedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">modele</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cv_results</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">cv_results</span><span class="p">[[</span><span class="s2">&quot;train_score&quot;</span><span class="p">,</span> <span class="s2">&quot;test_score&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_53_0.png" src="../_images/file_01_53_0.png" />
</div>
</div>
<p>Nous pouvons donc constater que notre modèle nous permet d’obtenir de bonnes
prédictions. Nous pouvons inspecter les coefficients des différents modèles
obtenus pendant la validation croisée.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coef</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span><span class="n">modele_cv</span><span class="o">.</span><span class="n">coef_</span> <span class="k">for</span> <span class="n">modele_cv</span> <span class="ow">in</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;estimator&quot;</span><span class="p">]],</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">nom_colonnes</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">box</span><span class="p">(</span><span class="n">vert</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">whis</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Coefficients des modèles linéaires </span><span class="se">\n</span><span class="s2">obtenus par validation croisée&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_55_0.png" src="../_images/file_01_55_0.png" />
</div>
</div>
<p>Nous pouvons donc observer en utilisant une représentation graphique sous
forme de boîte à moustache que les coefficients des modèles obtenus sont très
proches de ceux de la combinaison linéaire.</p>
<p>So far, so good! Nous allons mainteant introduire des <strong>variables
collinéaires</strong>. Une variable collinéaire est une variable qui sera corrélé
avec une autre. Nous allons analyser le type de problème engendré par ce type
de variables lorsque un modèle linéaire utilisant les moindres carrés sera
appliqué.</p>
<p>Pour cela, nous allons répéter plusieurs fois les variables prédictives
plusieurs fois.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Nous allons répéter l’expérience précédente et représenter graphiquement
les coefficients des modèles obtenus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span> <span class="o">=</span> <span class="n">RepeatedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">modele</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nom_colonnes</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Colonne #</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">coef</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span><span class="n">modele_cv</span><span class="o">.</span><span class="n">coef_</span> <span class="k">for</span> <span class="n">modele_cv</span> <span class="ow">in</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;estimator&quot;</span><span class="p">]],</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">nom_colonnes</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">box</span><span class="p">(</span><span class="n">vert</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">whis</span><span class="o">=</span><span class="mf">1e15</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Coefficients des modèles linéaires </span><span class="se">\n</span><span class="s2">obtenus par validation croisée&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_60_0.png" src="../_images/file_01_60_0.png" />
</div>
</div>
<p>Nous pouvons remarquer que les coefficients des modèles obtenus sont
extrêmement élevés et complètement dissociés des valeurs originales. Ce
problème est lié à une imprécision des calculs numériques.</p>
<p>Nous pouvons aller un peu plus loin dans les détails en rappelant l’équation
Normale pour résoudre notre système d’équations linéaires :</p>
<div class="math notranslate nohighlight">
\[
\beta = (X^T X)^{-1} X^T y
\]</div>
<p>Dans cette équation, nous pouvons voir que nous devons inverser la matrice de
Gram <span class="math notranslate nohighlight">\((X^T X)\)</span>. Si cette matrice n’est pas inversible, nous serions donc dans
l’impossibilité de calculer les coefficients du modèle. Une manière de
vérifier est de calculer le déterminant de cette matrice.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0
</pre></div>
</div>
</div>
</div>
<p>Le déterminant étant nul, il n’est donc pas possible d’inverser la matrice :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
<span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Singular matrix
</pre></div>
</div>
</div>
</div>
<p>En pratique, cette matrice n’utilise pas strictement la fonction inverse
ci-dessus ce qui explique le réultat obtenu qui est cependant incorrect.</p>
<p>Afin de résoudre ce problème, nous pouvons introduire le principe de
régularisation où l’idée est d’ajouter à la fonction de coût un terme
permettant de contraindre d’une manière donnée (i.e. qui dépend du type de
régularisation) la valeur des coefficients.</p>
<p>Nous allons tout d’abord formuler la régularisation de type L2 :</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\beta) = \frac{1}{N} \sum_{i=1}^N \left( y_i - f(X_i, \beta)
\right)^2 + \alpha \||\beta\||_2^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> est le paramètre controllant l’impact de la régularisation.</p>
<p>En analysant la fonction de coût, nous pouvons remarquer que si le paramètre
<span class="math notranslate nohighlight">\(\alpha\)</span> est nul, le terme contraignant les coefficients sera nul et donc
nous avons une simple régression linéaire. En revanche, si <span class="math notranslate nohighlight">\(\alpha\)</span> est
élevé, nous allons contraindre les coefficients plutôt que de réduire le
terme correspondant à l’erreur quadratique. Donc nous obtiendrons un modèle
qui minimisera la valeur des coefficients.</p>
<p>Ce type de modèle est appelé <strong>Ridge regression</strong>. Nous allons le mettre de
suite en oeuvre en utilisant <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> en répétant l’expérience
précédente.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">modele</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">RepeatedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">modele</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nom_colonnes</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Colonne #</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">coef</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span><span class="n">modele_cv</span><span class="o">.</span><span class="n">coef_</span> <span class="k">for</span> <span class="n">modele_cv</span> <span class="ow">in</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;estimator&quot;</span><span class="p">]],</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">nom_colonnes</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">box</span><span class="p">(</span><span class="n">vert</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">whis</span><span class="o">=</span><span class="mf">1e15</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Coefficients des modèles linéaires </span><span class="se">\n</span><span class="s2">obtenus par validation croisée&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_67_0.png" src="../_images/file_01_67_0.png" />
</div>
</div>
</div>
<div class="section" id="importance-du-pretraitement">
<h3>Importance du prétraitement<a class="headerlink" href="#importance-du-pretraitement" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="resolution-de-problemes-non-lineaires">
<h2>Résolution de problèmes non-linéaires<a class="headerlink" href="#resolution-de-problemes-non-lineaires" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="quantifier-l-incertitude-des-predictions">
<h2>Quantifier l’incertitude des prédictions<a class="headerlink" href="#quantifier-l-incertitude-des-predictions" title="Permalink to this headline">¶</a></h2>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapter_00"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="file_00.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Introduction</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="file_02.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Famille des modèles non-paramétriques</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Cédric Lemaître<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>