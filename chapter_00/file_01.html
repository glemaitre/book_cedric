
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Famille des modèles paramétriques &#8212; Cédric&#39;s book</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Famille des modèles non-paramétriques" href="file_02.html" />
    <link rel="prev" title="Introduction" href="file_00.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Cédric's book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../landing-page.html">
   Table of content
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="chapter_00_intro.html">
   Apprentisage automatique supervisé
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="file_00.html">
     Introduction
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Famille des modèles paramétriques
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="file_02.html">
     Famille des modèles non-paramétriques
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="file_03.html">
     Recherche des hyperparamètres
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="file_04.html">
     Gestion des variables catégorielles
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter_99/chapter_99_intro.html">
   Real-world examples
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter_99/file_00.html">
     Analyse de statistique tradionnelle
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/chapter_00/file_01.py"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.py</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modele-lineaire">
   Modèle linéaire
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#trouver-le-meilleur-modele-possible">
     Trouver le meilleur modèle possible
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#alternative-a-la-methode-des-moindres-carres">
     Alternative à la méthode des moindres carrés
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principe-de-la-regularisation">
     Principe de la régularisation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#statuer-le-probleme">
       Statuer le problème
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#regularisation-l2">
       Régularisation L2
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#utiliser-a-la-foid-regularisation-l1-et-l2">
       Utiliser à la foid regularisation L1 et L2
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#de-la-regression-a-la-classification">
     De la régression à la classification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#alternative-en-utilisant-une-approche-geometrique">
     Alternative en utilisant une approche géométrique
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#note-concernant-le-parametre-de-regularisation">
     Note concernant le paramètre de régularisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#importance-du-pretraitement">
     Importance du prétraitement
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#effet-des-donnees-non-traitees">
       Effet des données non-traîtées
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pretraitement-integre-dans-scikit-learn">
       Prétraitement intégré dans
       <code class="docutils literal notranslate">
        <span class="pre">
         scikit-learn
        </span>
       </code>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#probleme-de-convergence">
       Problème de convergence
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resolution-de-problemes-non-lineaires">
   Résolution de problèmes non-linéaires
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quantifier-l-incertitude-des-predictions">
   Quantifier l’incertitude des prédictions
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="famille-des-modeles-parametriques">
<h1>Famille des modèles paramétriques<a class="headerlink" href="#famille-des-modeles-parametriques" title="Permalink to this headline">¶</a></h1>
<p>Dans le chapitre précédent, nous avons détaillé le principe d’un modèle
prédictif de manière mathématique. Nous pouvons rappeler cette formulation :</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f(X)
\]</div>
<p>Nous avons même donné un exemple d’un modèle assez naif ou nous avons utilisé
une relation entre notre variable d’entrée et notre variable de sortie. Cette
manière de définir un modèle est nommée <strong>modèle paramétrique</strong>. En effet,
nous avons défini un modèle paramétrique de la forme suivante :</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f(X) = X \beta
\]</div>
<p>Le paramètre <span class="math notranslate nohighlight">\(\beta\)</span> est donc le <strong>paramètre</strong> de notre modèle. L’idée
derrière cette famille de modèles est donc de pouvoir compresser
l’information de notre jeu de données d’apprentissage avec seulement quelques
paramètres et un <em>apriori</em> sur la relation entre <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(y\)</span> (nous reviendrons
plus en détail sur cet aspect dans les sections qui viennent).</p>
<p>Dans ce chapitre, nous allons tout d’abord détailler une des familles les
plus simple : les modèles linéaires. Nous présenterons certaines composantes
importantes de ce type de modèle. Par la suite, nous montrerons que ces
modèles peuvent également utilisés pour des problèmes non-linéaires.</p>
<div class="section" id="modele-lineaire">
<h2>Modèle linéaire<a class="headerlink" href="#modele-lineaire" title="Permalink to this headline">¶</a></h2>
<p>Un modèle linéaire est un modèle paramétrique qui est défini par une relation
entre <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(y\)</span> tel que <span class="math notranslate nohighlight">\(y\)</span> est une combination linéaire de <span class="math notranslate nohighlight">\(X\)</span>. Notre
modèle dans le chapitre précédent était un modèle linéaire :</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f(X, \beta) = X \beta
\]</div>
<p>Le terme “combination” nous indique que nous pouvons généraliser cette
relation en combinant toutes les variables d’entrée (i.e. colonnes) de <span class="math notranslate nohighlight">\(X\)</span>.
Un tel modèle est donc défini de la manière suivante :</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f(X, \beta) = X \beta = \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n
X_n
\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta_n\)</span> est le paramètre associé à la variable <span class="math notranslate nohighlight">\(X_n\)</span>. La relation
ci-dessus force notre modèle de prédire 0 lorsque les valeurs dans <span class="math notranslate nohighlight">\(X\)</span> sont
également à 0. Pour avoir plus de flexibilité, un paramètre <span class="math notranslate nohighlight">\(\beta_0\)</span> est
utilisé pour représenter cette constante et est appelé l’<strong>intercept</strong>.</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = f(X, \beta) = X \beta = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... +
\beta_n X_n
\]</div>
<p>Nous pouvons donc comprendre que nous faisons un <em>apriori</em> entre le lien
entre <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(y\)</span> : nous pensons que <span class="math notranslate nohighlight">\(y\)</span> est une combinaison linéaire de <span class="math notranslate nohighlight">\(X\)</span>
et que cet relations est suffisante. Un peu plus tard dans ce chapitre, nous
donnerons des exemples où ce n’est pas le cas et où nous devrons modifier la
formulation de notre modèle.</p>
<p>En revanche, si cet <em>apriori</em> est correct, nous venons donc de compresser
notre de dataset de taille composé de <span class="math notranslate nohighlight">\(N\)</span> échantillons à un modèle de taille
<span class="math notranslate nohighlight">\(P + 1\)</span> paramètres (i.e. + 1 corresponds à l’intercept). Maintenant que nous
avons défini notre modèle, il nous est possible de trouver les paramètres.</p>
<div class="section" id="trouver-le-meilleur-modele-possible">
<h3>Trouver le meilleur modèle possible<a class="headerlink" href="#trouver-le-meilleur-modele-possible" title="Permalink to this headline">¶</a></h3>
<p>Maintenant que nous connaisons la paramétrisation de notre modèle, nous
pouvons l’illustrer sur le même jeu de données que nous avons utilisé dans
le chapitre précédent. Tout d’abord, nous chargeons les données.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">donnees</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../datasets/penguins_regression.csv&quot;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">donnees</span><span class="p">[[</span><span class="s2">&quot;Longueur Aileron (mm)&quot;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">donnees</span><span class="p">[</span><span class="s2">&quot;Masse Corporelle (g)&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Et nous pouvons visualaliser la relation entre <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(y\)</span> :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;poster&quot;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Masse corporelle en fonction de</span><span class="se">\n</span><span class="s2">la longueur d&#39;aileron&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_3_0.png" src="../_images/file_01_3_0.png" />
</div>
</div>
<p>Il existe une infinité de modèle linéaire qui pourraient être utilisés pour
pour prédire la masse corporelle de nos pingouins. Définissons une fonction
Python générique qui permet de prédire la masse corporelle de notre pinguoin
en fonction de la longueur d’aileron.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">modele_lineaire</span><span class="p">(</span><span class="n">longueur_aileron</span><span class="p">,</span> <span class="n">parametres</span><span class="p">):</span>
    <span class="c1"># notre modèle est défini par: y = beta_0 + x_1 * beta_1</span>
    <span class="k">return</span> <span class="n">parametres</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">parametres</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">longueur_aileron</span>
</pre></div>
</div>
</div>
</div>
<p>Maintenant que nous avons notre modèle, nous pouvons visualiser quelques
modèles avec différents paramètres.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions_modele_1</span> <span class="o">=</span> <span class="n">modele_lineaire</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">3_000</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
<span class="n">predictions_modele_2</span> <span class="o">=</span> <span class="n">modele_lineaire</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">6_000</span><span class="p">,</span> <span class="mi">50</span><span class="p">])</span>
<span class="n">predictions_modele_3</span> <span class="o">=</span> <span class="n">modele_lineaire</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">2_000</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_modele_1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Modèle #1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_modele_2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Modèle #2&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_modele_3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Modèle #3&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Quel modèle est le meilleur?&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_8_0.png" src="../_images/file_01_8_0.png" />
</div>
</div>
<p>A partir de ce graphique, la question que nous pourrions avoir est de savoir
quel modèle est le meilleur. Qualitativement, nous pourrions dire que le
modèle #1 est le pire modèle. Entre le modèle #2 et #3, nous pourrions
préviligier le modèle #2 car il semble plus “centré” avec nos données.</p>
<p>Cependant, choisir un modèle ne peut-être basé sur une évaluation
qualitative. Dans le chapitre précédent, nous avons utilisé différentes
méthodes qui calculaient une erreur. Nous pouvons ici calculer une erreur
donnée : l’erreur quadratique moyenne.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Erreur du modèle #1: </span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_modele_1</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Erreur du modèle #2: </span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_modele_2</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Erreur du modèle #3: </span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_modele_3</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Erreur du modèle #1: 1609923.83
Erreur du modèle #2: 178899.85
Erreur du modèle #3: 261327.34
</pre></div>
</div>
</div>
</div>
<p>En utilisant cette erreur, nous avons la confirmation que le modèle #2 a la
plus petite erreur. En revanche, est ce que ce modèle est le meilleur
possible ? Si non, comment pouvons nous trouver un modèle la plus faible
possible ?</p>
<p>Nous donc un probème d’optimisation où nous voudrions minimiser cette erreur
également appelée <strong>fonction de coût</strong> dans ce contexte. Donc, nous pouvons
donc définir notre fonctionde coup comme l’erreur quadratique moyenne
formulée ci-dessous :</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\beta) = \frac{1}{N} \sum_{i=1}^N \left( y_i - f(X_i, \beta)
\right)^2
\]</div>
<p>Et nous chercherons donc à minimiser cette fonction de coût. En d’autre
termes, nous serions intéressés par trouver le minimum de
<span class="math notranslate nohighlight">\(\mathcal{L}(\beta)\)</span>. Cette minimisation est également appellée méthode
des moindres carrés.</p>
<div class="math notranslate nohighlight">
\[
\min_{\beta} \mathcal{L}(\beta)
\]</div>
<p>Trouver le minimum d’une fonction donnée est un problème typique
d’<strong>optimisation methématique</strong> et il existe plusieurs méthodes, certaines
plus performantes que d’autres, dépendant de la fonction à minimiser. Nous
pouvons mentionner les méthodes basées sur le gradient qui nécessitent de
pouvoir dériver la fonction de coût.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Il existe une solution analytique pour la fonction de coût que nous avons
définie.</p>
<div class="math notranslate nohighlight">
\[
\beta = \left( X^T X \right)^{-1} X^T y
\]</div>
<p>En revanche, nous avons introduit la méthode de gradient car elle nous
permettra d’avoir une certaine réflexion concernant les futurs fonctions de
coût que nous allons définir.</p>
</div>
<p>Nous avons la chance que notre fonction de coût définie comme l’erreur
quadratique moyenne soit facilement dérivable. Il serait donc facile de
calculer le mimimum de cette fonction de coût.</p>
<p><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> nous propose une classe dénommée <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> qui
permet de minimser cette fonction de coût. Nous allons la mettre en pratique
dès maintenant.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">modele</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">modele</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearRegression()
</pre></div>
</div>
</div>
</div>
<p>Maintenant que notre modèle est entrainé, nous pouvons l’utiliser pour
observer visuellement quels sont les prédictions produites par ce modèle.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">modele</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Regression linéaire&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Modele LinearRegression&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_15_0.png" src="../_images/file_01_15_0.png" />
</div>
</div>
<p>Ce modèle semble donc bien minimiser la fonction de coût et est
qualititivement correct. Nous pouvons donc maintenant nous pencher sur notre
modèle et obtenir la valeurs des paramètres.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modele</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([49.68556641])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modele</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-5780.831358077066
</pre></div>
</div>
</div>
</div>
<p>Nous pouvons donc observer deux attributs qui correspondent aux paramètres de
notre modèle. <code class="docutils literal notranslate"><span class="pre">coef_</span></code> contient les paramètres de <span class="math notranslate nohighlight">\(\beta_1, ..., \beta_n\)</span>
alors que <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> contient le paramètre de <span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Dans scikit-learn, les attributs finissant par <code class="docutils literal notranslate"><span class="pre">_</span></code> sont des attributs
qui sont créés après avoir appelé la méthode <code class="docutils literal notranslate"><span class="pre">fit()</span></code>. Ils sont liés à
l’algorithme d’apprentissage et seront nécessaires pour pouvoir créer
des prédictions.</p>
</div>
<p>Maintenant, nous pouvons interpréter la valeurs des paramètres de notre
modèle. La valeur dans la variable <code class="docutils literal notranslate"><span class="pre">coef_</span></code> est la valeur associé à la
variable “Longueur Aileron (mm)”. Cette valeur correspond à l’incrément de la
masse corporelle lorsqu’un pingouin à un incrément de 1 mm de longueur
d’aileron. Dans notre cas, cette valeur est d’environ 50 grammes. La valeur
de la variable <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> correspond à la valeur de l’ordonnée à l’origine
: un pinguoin avec un aileron de 0 mm aura une masse corporelle de -5781
grammes! Cette valeur est beaucoup plus compliquée à comprendre mais elle
nous permet d’avoir un modèle plus flexible, ne passant pas par l’origine.</p>
</div>
<div class="section" id="alternative-a-la-methode-des-moindres-carres">
<h3>Alternative à la méthode des moindres carrés<a class="headerlink" href="#alternative-a-la-methode-des-moindres-carres" title="Permalink to this headline">¶</a></h3>
<p>La méthode des moindres carrés est la méthode la plus simple que nou pouvions
présenter. En revanche, cette méthode à des limitations connues. Nous allons
en présenter l’une d’entre elles, ainsi montrer comment nous pouvons la
contourner.</p>
<p>Nous allons reprendre les mêmes données que précédemment utilisées. En
revanche, nous allons simuler que des erreurs de saisie sont survenues lors
de la collecte des données. Pour cela, nous allons rajouter un échantillon de
pinguoin qui aura un aileron de longueur de 230 mm et une masse corporelle de
300 grammes. Cette erreur pourrait être dûe à une erreur de saisie ou un 0
est manquant.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">donnees</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;Longueur Aileron (mm)&quot;</span><span class="p">:</span> <span class="mi">230</span><span class="p">,</span> <span class="s2">&quot;Masse Corporelle (g)&quot;</span><span class="p">:</span> <span class="mi">300</span><span class="p">},</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">donnees</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">donnees</span><span class="p">[[</span><span class="s2">&quot;Longueur Aileron (mm)&quot;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">donnees</span><span class="p">[</span><span class="s2">&quot;Masse Corporelle (g)&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Nos données avec une</span><span class="se">\n</span><span class="s2">erreur de saisie&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_21_0.png" src="../_images/file_01_21_0.png" />
</div>
</div>
<p>Nous pouvons observer que nous avons un nouveau échantillon dans le cadrant
en bas à droite de notre graphique. Nous allons maintenant entraîner un
modèle linéaire qui minimise l’erreur quadratique moyenne. Pour simuler que
nous avons plusieurs erreurs de saisie, nous allons entraîner notre modèle en
assignant des poids à chaque échantillon : tous les échantillons auront un
poids de 1, sauf le nouvel échantillon qui aura un poids de 10.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">poids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
<span class="n">poids</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modele</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">predictions_err_quadratique</span> <span class="o">=</span> <span class="n">modele</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">poids</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_err_quadratique</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Regression linéaire&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Modele LinearRegression&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_25_0.png" src="../_images/file_01_25_0.png" />
</div>
</div>
<p>Nous pouvons donc observer que le faite d’avoir des erreurs de saisie à une
influence non négligeable sur la qualité de notre modèle. Ceci peut-être
expliqué par le type de fonction de coût que nous utilisons. Il sera plus
facile d’obtenir une intuition en représentant graphiquement cette fonction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">erreur_quadratique</span><span class="p">(</span><span class="n">cible</span><span class="p">,</span> <span class="n">prediction</span><span class="p">):</span>
    <span class="n">cout</span> <span class="o">=</span> <span class="p">(</span><span class="n">cible</span> <span class="o">-</span> <span class="n">prediction</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">cout</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span> <span class="o">=</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">erreur_quadratique</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xx</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Erreur quadratique&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_28_0.png" src="../_images/file_01_28_0.png" />
</div>
</div>
<p>Nous pouvons donc observer que le faite d’élever au carré l’erreur pénalise
extrêment les échantillons avec une grande erreur. Il serait donc intéressant
d’utiliser une fonction de coût qui affectera un coût moindre aux
échantillons pour lesquel notre modèle commet le plus d’erreur. Au lieu de
prendre le carré de l’erreur, nous pourrions seulement utiliser la valeur
absolue de l’erreur. Cette erreur sera donc l’<strong>erreur absolute moyenne</strong> et
peut-être formulée comme suit :</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\beta) = \frac{1}{N} \sum_{i=1}^N \left| y_i - f(X_i, \beta)
\right|
\]</div>
<p>Nous pouvons comparer visualement la représentation de cette fonction de coût
avec la représentation de la fonction d’erreur quadratique.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">erreur_absolue</span><span class="p">(</span><span class="n">cible</span><span class="p">,</span> <span class="n">prediction</span><span class="p">):</span>
    <span class="n">cout</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">cible</span> <span class="o">-</span> <span class="n">prediction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cout</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">erreur_quadratique</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xx</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Erreur quadratique&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">erreur_absolue</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xx</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Erreur absolue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Comparaison des fonctions de coût&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_31_0.png" src="../_images/file_01_31_0.png" />
</div>
</div>
<p>On peut donc observer que l’erreur absolue pénalisera les échantillons avec
une grande erreur. En revanche, si nous nous attardons sur l’erreur aboslue
nous pouvons observer quelle n’est pas dérivable en 0. Ceci nous empêche
d’utiliser une méthode d’optimisation basée sur le gradient ce qui est
problématique.</p>
<p>Si nous voulons utiliser une méthode par descente de gradient, il est donc
nécessaire de trouver un moyen de combiner les deux fonctions de coût :
utiliser l’erreur absolue loin de 0 pour moins pénaliser les échantillons
avec une grande erreur et utiliser l’erreur quadratique quand l’erreur est
proche de 0 pour que nous puissions déterminer le gradient.</p>
<p>Cette fonction de coût est connu sous le nom de la fonction de Huber et est
formulée comme suit :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{L}(\beta) = \frac{1}{N} \sum^{N}_{i=1} \begin{cases}
\left( y_i - f(X_i, \beta) \right)^2 &amp; \text{si } \left|
y_i - f(X_i, \beta) \right| &lt; \epsilon \\
2 \epsilon \left( | y_i - f(X_i, \beta) | - \epsilon^2 \right) &amp; \text{sinon}
\end{cases}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fonction_huber</span><span class="p">(</span><span class="n">cible</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.35</span><span class="p">):</span>
    <span class="n">cout_absolue</span> <span class="o">=</span> <span class="n">erreur_absolue</span><span class="p">(</span><span class="n">cible</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
    <span class="n">cout_quadratique</span> <span class="o">=</span> <span class="n">erreur_quadratique</span><span class="p">(</span><span class="n">cible</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>

    <span class="n">plus_grand_epsilon</span> <span class="o">=</span> <span class="n">cout_absolue</span> <span class="o">&gt;</span> <span class="n">epsilon</span>

    <span class="n">cout</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
    <span class="n">cout</span><span class="p">[</span><span class="o">~</span><span class="n">plus_grand_epsilon</span><span class="p">]</span> <span class="o">=</span> <span class="n">cout_quadratique</span><span class="p">[</span><span class="o">~</span><span class="n">plus_grand_epsilon</span><span class="p">]</span>
    <span class="n">cout</span><span class="p">[</span><span class="n">plus_grand_epsilon</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="mi">2</span> <span class="o">*</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">cout_absolue</span><span class="p">[</span><span class="n">plus_grand_epsilon</span><span class="p">]</span> <span class="o">-</span> <span class="n">epsilon</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">cout</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">erreur_quadratique</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xx</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Erreur quadratique&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">erreur_absolue</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xx</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Erreur absolue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">fonction_huber</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xx</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Fonction de Huber&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Comparaison des fonctions de coût&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_34_0.png" src="../_images/file_01_34_0.png" />
</div>
</div>
<p>Nous pouvons donc observer que la fonction de Huber a les avantages des deux
fonctions de coût précédentes. <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> propose une classe appelée
<code class="docutils literal notranslate"><span class="pre">HuberRegressor</span></code> qui permettra d’optimiser cette fonction de coût. Nous
allons donc utiliser ce modèle sur notre jeu de données et observer la
différence sur les prédictions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">HuberRegressor</span>

<span class="n">modele</span> <span class="o">=</span> <span class="n">HuberRegressor</span><span class="p">()</span>
<span class="n">predictions_err_huber</span> <span class="o">=</span> <span class="n">modele</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">poids</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_err_quadratique</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Quadratique&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_err_huber</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Huber&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Comparaison modèle linéaire&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_37_0.png" src="../_images/file_01_37_0.png" />
</div>
</div>
<p>Nous pouvons donc constater que le modèle linéaire minimisant la fonction de
Huber permet d’obtenir un meilleur modèle que celui minimisant la fonction
de coût quadratique.</p>
<p>Pour confirmer de manière quantitative, nous pourrions calculer des erreurs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Modèle linéaire quadratique:</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;  Erreur quadratique moyenne : &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_err_quadratique</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;  Erreur absolue moyenne : &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_err_quadratique</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Modèle linéaire Huber:</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;  Erreur quadratique moyenne : &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_err_huber</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="s2">&quot;  Erreur absolue moyenne : &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">predictions_err_huber</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Modèle linéaire quadratique:
  Erreur quadratique moyenne : 313313.17
  Erreur absolue moyenne : 412.61
Modèle linéaire Huber:
  Erreur quadratique moyenne : 236451.75
  Erreur absolue moyenne : 328.44
</pre></div>
</div>
</div>
</div>
<p>Nous pouvons confirmer que le modèle linéaire quadratique a une erreur
quadratique moins élevèe que le modèle linéaire Huber. En revanche, nous
avons la conclusion opposée pour l’erreur absolue.</p>
<p>Il est quand même intéressant de mentionner qu’il es possible d’optimiser la
fonction de coût basée sur l’erreur absolue. En revanche, la méthode
d’optimisation sera différente. Ce type de d’estimateur est connue en anglais
sous le nom de “Least Absolute Deviation” (LAD). Cet estimateur minimisera
donc la fonction de coût des erreurs absolues et sera un estimateur de la
médiane de nos données. <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> propose une classe appelée
<code class="docutils literal notranslate"><span class="pre">QuantileRegressor</span></code> qui permet de regresser n’importe quelle quantile et
notemment la médiane.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">QuantileRegressor</span>

<span class="n">modele</span> <span class="o">=</span> <span class="n">QuantileRegressor</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;highs&quot;</span><span class="p">)</span>
<span class="n">predictions_err_absolue</span> <span class="o">=</span> <span class="n">modele</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">poids</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">donnees</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_err_quadratique</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Quadratique&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_err_huber</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Huber&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">predictions_err_absolue</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Absolue&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Comparaison modèle linéaire&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_42_0.png" src="../_images/file_01_42_0.png" />
</div>
</div>
<p>Nous pouvons donc constater que l’estimateur basé sur la fonction de coût de
Huber est très proche de l’estimateur de la médiane. Nous reviendrons plus
tard dans ce chapitre sur l’estimateur quantiles pour estimer des intervalles
de confiance autour de la médiane.</p>
</div>
<div class="section" id="principe-de-la-regularisation">
<h3>Principe de la régularisation<a class="headerlink" href="#principe-de-la-regularisation" title="Permalink to this headline">¶</a></h3>
<div class="section" id="statuer-le-probleme">
<h4>Statuer le problème<a class="headerlink" href="#statuer-le-probleme" title="Permalink to this headline">¶</a></h4>
<p>Dans la section précédente, nous avons abordé le principe de la fonction de
coût. Nous avons étudié et analysé différentes types d’erreur. En revanche,
nous n’avons pas abordé les différentes formes de régularisation.</p>
<p>Dans cette section, nous allons dans un premier temps motivé l’utilité de la
régularisation. Dans un deuxième temps, nous allons présenter les différents
types de régularisation.</p>
<p>Pour motiver l’utilité de la régularisation, nous allons générer un jeu de
données simple que nous allons pouvoir facilement interpréter. Nous allons
générer une cible à partir d’une combination linéaire de deux variables. Nous
allons également ajouter trois variables qui ne seront pas prédictives (i.e.
aucun lien n’existera entre la cible <code class="docutils literal notranslate"><span class="pre">y</span></code> et ces variables). Pour rendre le
problème plus réaliste, nous allons ajouter un bruit Gaussien à la cible.</p>
<p>La fonction <code class="docutils literal notranslate"><span class="pre">make_regression</span></code> dans <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> nous permet de générer un
tel jeu de données.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>

<span class="n">n_features</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">true_coef</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span>
    <span class="n">n_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
    <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">coef</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">noise</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>En plus des données <code class="docutils literal notranslate"><span class="pre">X</span></code> et <code class="docutils literal notranslate"><span class="pre">y</span></code>, cette fonction nous fournit également les
coefficients de la combinaison linéaire. Nous pouvons visualiser ces
coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nom_colonnes</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Colonne #</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">)]</span>
<span class="n">true_coef</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">true_coef</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">nom_colonnes</span><span class="p">)</span>
<span class="n">true_coef</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">barh</span><span class="p">()</span>
<span class="n">true_coef</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Colonne #0    44.145196
Colonne #1    63.006149
Colonne #2     0.000000
Colonne #3     0.000000
Colonne #4     0.000000
Colonne #5     0.000000
dtype: float64
</pre></div>
</div>
<img alt="../_images/file_01_47_1.png" src="../_images/file_01_47_1.png" />
</div>
</div>
<p>Nous pouvons également visualiser les liens marginales entre la cible <code class="docutils literal notranslate"><span class="pre">y</span></code> et
chacune des variables composant notre jeu de données <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">donnees</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">nom_colonnes</span><span class="p">)</span>
<span class="n">donnees</span><span class="p">[</span><span class="s2">&quot;Cible&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
<span class="n">donnees</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="s2">&quot;Cible&quot;</span><span class="p">,</span> <span class="n">var_name</span><span class="o">=</span><span class="s2">&quot;Variable&quot;</span><span class="p">,</span> <span class="n">value_name</span><span class="o">=</span><span class="s2">&quot;Valeur&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Valeur&quot;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Cible&quot;</span><span class="p">,</span>
    <span class="n">col</span><span class="o">=</span><span class="s2">&quot;Variable&quot;</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;Variable&quot;</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">donnees</span><span class="p">,</span>
    <span class="n">col_wrap</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">ci</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">scatter_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;s&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_50_0.png" src="../_images/file_01_50_0.png" />
</div>
</div>
<p>Sur ce graphique, nous pouvons donc confirmer qu’il existe une lien entre les
deux premières variables et notre cible alors qu’il n’existe aucun lien avec
les autres variables.</p>
<p>Maintenant, que nous avons quelques intuitions concernant notre jeu de
données, nous allons créer un modèle linéaire pour estimer les coefficients
de la combinaison linéaire. Mais tout d’abord, nous allons évaluer un tel
modèle via une validation croisée. Nous utiliserons le score <span class="math notranslate nohighlight">\(R^2\)</span> pour
evaluer notre modèle (ce score est une mesure de la qualité du modèle où le
maximum score est 1).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RepeatedKFold</span>

<span class="n">modele</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">RepeatedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">modele</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cv_results</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">cv_results</span><span class="p">[[</span><span class="s2">&quot;train_score&quot;</span><span class="p">,</span> <span class="s2">&quot;test_score&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_53_0.png" src="../_images/file_01_53_0.png" />
</div>
</div>
<p>Nous pouvons donc constater que notre modèle nous permet d’obtenir de bonnes
prédictions. Nous pouvons inspecter les coefficients des différents modèles
obtenus pendant la validation croisée.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coef</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span><span class="n">modele_cv</span><span class="o">.</span><span class="n">coef_</span> <span class="k">for</span> <span class="n">modele_cv</span> <span class="ow">in</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;estimator&quot;</span><span class="p">]],</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">nom_colonnes</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">box</span><span class="p">(</span><span class="n">vert</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">whis</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Coefficients des modèles linéaires </span><span class="se">\n</span><span class="s2">obtenus par validation croisée&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_55_0.png" src="../_images/file_01_55_0.png" />
</div>
</div>
<p>Nous pouvons donc observer en utilisant une représentation graphique sous
forme de boîte à moustache que les coefficients des modèles obtenus sont très
proches de ceux de la combinaison linéaire.</p>
<p>So far, so good! Nous allons mainteant introduire des <strong>variables
collinéaires</strong>. Une variable collinéaire est une variable qui sera corrélé
avec une autre. Nous allons analyser le type de problème engendré par ce type
de variables lorsque un modèle linéaire utilisant les moindres carrés sera
appliqué.</p>
<p>Pour cela, nous allons répéter plusieurs fois les variables prédictives
plusieurs fois.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Nous allons répéter l’expérience précédente et représenter graphiquement
les coefficients des modèles obtenus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span> <span class="o">=</span> <span class="n">RepeatedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">modele</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nom_colonnes</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Colonne #</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">coef</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span><span class="n">modele_cv</span><span class="o">.</span><span class="n">coef_</span> <span class="k">for</span> <span class="n">modele_cv</span> <span class="ow">in</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;estimator&quot;</span><span class="p">]],</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">nom_colonnes</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">box</span><span class="p">(</span><span class="n">vert</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">whis</span><span class="o">=</span><span class="mf">1e15</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Coefficients des modèles linéaires </span><span class="se">\n</span><span class="s2">obtenus par validation croisée&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_60_0.png" src="../_images/file_01_60_0.png" />
</div>
</div>
<p>Nous pouvons remarquer que les coefficients des modèles obtenus sont
extrêmement élevés et complètement dissociés des valeurs originales. Ce
problème est lié à une imprécision des calculs numériques.</p>
<p>Nous pouvons aller un peu plus loin dans les détails en rappelant l’équation
Normale pour résoudre notre système d’équations linéaires :</p>
<div class="math notranslate nohighlight">
\[
\beta = (X^T X)^{-1} X^T y
\]</div>
<p>Dans cette équation, nous pouvons voir que nous devons inverser la matrice de
Gram <span class="math notranslate nohighlight">\((X^T X)\)</span>. Si cette matrice n’est pas inversible, nous serions donc dans
l’impossibilité de calculer les coefficients du modèle. Une manière de
vérifier est de calculer le déterminant de cette matrice.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0
</pre></div>
</div>
</div>
</div>
<p>Le déterminant étant nul, il n’est donc pas possible d’inverser la matrice :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>
<span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Singular matrix
</pre></div>
</div>
</div>
</div>
<p>En pratique, cette matrice n’utilise pas strictement la fonction inverse
ci-dessus ce qui explique le réultat obtenu qui est cependant incorrect.</p>
</div>
<div class="section" id="regularisation-l2">
<h4>Régularisation L2<a class="headerlink" href="#regularisation-l2" title="Permalink to this headline">¶</a></h4>
<p>Afin de résoudre ce problème, nous pouvons introduire le principe de
régularisation où l’idée est d’ajouter à la fonction de coût un terme
permettant de contraindre d’une manière donnée (i.e. qui dépend du type de
régularisation) la valeur des coefficients.</p>
<p>Nous allons tout d’abord formuler la régularisation de type L2 :</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\beta) = \frac{1}{N} \sum_{i=1}^N \left( y_i - f(X_i, \beta)
\right)^2 + \alpha \|\beta\|_2^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> est le paramètre controllant l’impact de la régularisation.</p>
<p>En analysant la fonction de coût, nous pouvons remarquer que si le paramètre
<span class="math notranslate nohighlight">\(\alpha\)</span> est nul, le terme contraignant les coefficients sera nul et donc
nous avons une simple régression linéaire. En revanche, si <span class="math notranslate nohighlight">\(\alpha\)</span> est
élevé, nous allons contraindre les coefficients plutôt que de réduire le
terme correspondant à l’erreur quadratique. Donc nous obtiendrons un modèle
qui minimisera la valeur des coefficients.</p>
<p>Ce type de modèle est appelé <strong>Ridge regression</strong>. Nous allons le mettre de
suite en oeuvre en utilisant <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> en répétant l’expérience
précédente.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">modele</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">RepeatedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">modele</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nom_colonnes</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Colonne #</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">coef</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span><span class="n">modele_cv</span><span class="o">.</span><span class="n">coef_</span> <span class="k">for</span> <span class="n">modele_cv</span> <span class="ow">in</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;estimator&quot;</span><span class="p">]],</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">nom_colonnes</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">box</span><span class="p">(</span><span class="n">vert</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">whis</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Coefficients des modèles linéaires </span><span class="se">\n</span><span class="s2">obtenus par validation croisée&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_67_0.png" src="../_images/file_01_67_0.png" />
</div>
</div>
<p>Nous pouvons remarquer que nos modèles ne souffrent pas d’instabilité
numérique comme nous l’avons constaté lors de l’usage du modèle
<code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>. Nous pouvons même observer la correspondance entre les
coefficients des modèles <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> et les coefficients utilisés pour générer
les données originales. En effet, puisque que les variables prédictives
originales sont répétées trois fois, les coefficients des modèles <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>
sont trois fois moins importantes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">true_coef</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Colonne #0    44.145196
Colonne #1    63.006149
Colonne #2     0.000000
Colonne #3     0.000000
Colonne #4     0.000000
Colonne #5     0.000000
dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coef</span><span class="p">[</span><span class="n">true_coef</span><span class="o">.</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">*</span> <span class="mi">3</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Colonne #0    46.273055
Colonne #1    61.795346
Colonne #2    -0.509359
Colonne #3     0.018563
Colonne #4    -0.933461
Colonne #5     4.129136
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Nous allons maintenant confirmer expérimentalement, l’effet du paramètre
<span class="math notranslate nohighlight">\(\alpha\)</span> sur les coefficients des modèles <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>. Nous allons utiliser
une valeur très faible et une valeur très élevée.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-14</span><span class="p">,</span> <span class="mf">1e5</span><span class="p">]</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">):</span>
    <span class="n">modele</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="n">RepeatedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
        <span class="n">modele</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="n">coef</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="p">[</span><span class="n">modele_cv</span><span class="o">.</span><span class="n">coef_</span> <span class="k">for</span> <span class="n">modele_cv</span> <span class="ow">in</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;estimator&quot;</span><span class="p">]],</span>
        <span class="n">columns</span><span class="o">=</span><span class="n">nom_colonnes</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">coef</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">box</span><span class="p">(</span><span class="n">vert</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">whis</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Coefficients for $\alpha=</span><span class="si">{}</span><span class="s2">$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Effect of the parameter $\alpha$&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_72_0.png" src="../_images/file_01_72_0.png" />
</div>
</div>
<p>Nous pouvons donc confirmer nos premières intuitions. Plus <span class="math notranslate nohighlight">\(\alpha\)</span> augmente,
plus les coefficients des modèles <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> sont plus petits. Au contraire,
plus <span class="math notranslate nohighlight">\(\alpha\)</span> diminue, le modèle se rapproche d’une simple régression
linéaire, souffrant alors d’instabilité numérique.</p>
<p>Régularisation L1</p>
<p>Maintenant, au lieu d’utiliser la norme L2 comme contrainte imposer sur les
coefficients, nous pouvons utiliser une autre type de norme. Une possibilité
est d’utiliser la norme L1 :</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\beta) = \frac{1}{N} \sum_{i=1}^N \left( y_i - f(X_i, \beta)
\right)^2 + \alpha \|\beta\|_1
\]</div>
<p>Ce modèle est appelé <strong>Lasso regression</strong>. Nous pouvons obtenir les mêmes
intuitions que précédemment : lorsque <span class="math notranslate nohighlight">\(\alpha\)</span> est petit, le terme
contraignant les coefficients est négligeable et donc nous avons une simple
régression linéaire. Sinon, nous forcons la contraintes L1 sur les
coefficients. La question est maintenant de comprendre quel est l’effet de
minimizer la norme L1 sur les coefficients. Rien de tel que de répéter
l’expérience précédente pour le modèle <code class="docutils literal notranslate"><span class="pre">Lasso</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-14</span><span class="p">,</span> <span class="mf">1e4</span><span class="p">]</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">):</span>
    <span class="n">modele</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="n">RepeatedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
        <span class="n">modele</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="n">coef</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="p">[</span><span class="n">modele_cv</span><span class="o">.</span><span class="n">coef_</span> <span class="k">for</span> <span class="n">modele_cv</span> <span class="ow">in</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;estimator&quot;</span><span class="p">]],</span>
        <span class="n">columns</span><span class="o">=</span><span class="n">nom_colonnes</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">coef</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">box</span><span class="p">(</span><span class="n">vert</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">whis</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Coefficients for $\alpha=</span><span class="si">{}</span><span class="s2">$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Effect of the parameter $\alpha$&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.232e+04, tolerance: 5.513e+02
  model = cd_fast.enet_coordinate_descent(
</pre></div>
</div>
<img alt="../_images/file_01_74_1.png" src="../_images/file_01_74_1.png" />
</div>
</div>
<p>Nous pouvons constater que la minimiser la norme L1 forcera certains
coefficients à devenir nuls. Si le paramètre <span class="math notranslate nohighlight">\(\alpha\)</span> est trop grand, les
coefficients seront tous nul. Nous pouvons constater que le problème que nous
essayons de résoudre <code class="docutils literal notranslate"><span class="pre">Lasso</span></code> est performant car il permet de n’utiliser
seulement qu’une des variables prédictives et n’utilise pas les variables
colinéaires. En renvanche, il est intéressant que le choix de cette variable
parmi les variables colinéaires peut-être arbitraire.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modele</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">selection</span><span class="o">=</span><span class="s2">&quot;random&quot;</span><span class="p">)</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">RepeatedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">modele</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">coef</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span><span class="n">modele_cv</span><span class="o">.</span><span class="n">coef_</span> <span class="k">for</span> <span class="n">modele_cv</span> <span class="ow">in</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;estimator&quot;</span><span class="p">]],</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">nom_colonnes</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">box</span><span class="p">(</span><span class="n">vert</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">whis</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Effet du paramètre selection&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_76_0.png" src="../_images/file_01_76_0.png" />
</div>
</div>
<p>Dans ce graphique, nous pouvons observer que la variation des valeurs des
coefficients est réellement importante. Cela signifie que par exemple, la
colonne 0, 6 ou 8 peuvent être sélectionnées arbitrairement. Pour une
itération de validation croisée donnée, si la colonne 0 a été selectionnée,
alors <code class="docutils literal notranslate"><span class="pre">Lasso</span></code> ne sélectionnera pas la colonne 6 ou 8. Et ceci peut changer
d’itération à l’itération suivante. Utiliser le paramètre
<code class="docutils literal notranslate"><span class="pre">selection=&quot;cyclic&quot;</span></code> propose une sélection qui ne variera pas d’itération à
l’itération suivante.</p>
</div>
<div class="section" id="utiliser-a-la-foid-regularisation-l1-et-l2">
<h4>Utiliser à la foid regularisation L1 et L2<a class="headerlink" href="#utiliser-a-la-foid-regularisation-l1-et-l2" title="Permalink to this headline">¶</a></h4>
<p>Le dernier type de régularization que nous allons aborder est une combinaison
entre <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> et <code class="docutils literal notranslate"><span class="pre">Lasso</span></code>. La fonction de coût est alors définit comme suit :</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\beta) = \frac{1}{N} \sum_{i=1}^N \left( y_i - f(X_i, \beta)
\right)^2 + \alpha_1 \|\beta\|_1 + \alpha_2 \|\beta\|_2^2
\]</div>
<p>Ce modèle est appelé <strong>Elastic Net regression</strong>. Il permet alors de
sélectionner un sous-ensemble des variables en utilisant la norme L1 comme
dans <code class="docutils literal notranslate"><span class="pre">Lasso</span></code> et d’imposer une contrainte de type L2 sur les variables
restantes. <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> fournit un classe <code class="docutils literal notranslate"><span class="pre">ElasticNet</span></code> qui permet de
construire un tel modèle. En revanche, il n’expose pas deux paramètres
<span class="math notranslate nohighlight">\(\alpha_1\)</span> et <span class="math notranslate nohighlight">\(\alpha_2\)</span> mais un seul paramètre <span class="math notranslate nohighlight">\(\alpha\)</span> commun aux deux type
de contrainte L1 et L2 et un autre paramètre <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> qui permet de qui
permet de définir si nous favorison un modèle de type <code class="docutils literal notranslate"><span class="pre">Lasso</span></code> ou un modèle de
type <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">l1_ratios</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">l1_ratio</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">l1_ratios</span><span class="p">):</span>
    <span class="n">modele</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">)</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="n">RepeatedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
        <span class="n">modele</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="n">coef</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="p">[</span><span class="n">modele_cv</span><span class="o">.</span><span class="n">coef_</span> <span class="k">for</span> <span class="n">modele_cv</span> <span class="ow">in</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&quot;estimator&quot;</span><span class="p">]],</span>
        <span class="n">columns</span><span class="o">=</span><span class="n">nom_colonnes</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">coef</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">box</span><span class="p">(</span><span class="n">vert</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">whis</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coefficients for </span><span class="si">{</span><span class="n">l1_ratio</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Effect of the parameter l1_ratio</span><span class="se">\n</span><span class="s2"> dans ElasticNet&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_78_0.png" src="../_images/file_01_78_0.png" />
</div>
</div>
<p>Nous pouvons constater que quand le paramètre <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> est plus élevé,
alors nour avons un modèle qui force les variables random (e.g. colonne 2 à
5) à être nulle. Lorsque le paramètre <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> est plus faible, alors les
variables random (e.g. colonne 2 à 5) sont plus non nuls ce qui
correspondrait plus à un modèle de type <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>.</p>
</div>
</div>
<div class="section" id="de-la-regression-a-la-classification">
<h3>De la régression à la classification<a class="headerlink" href="#de-la-regression-a-la-classification" title="Permalink to this headline">¶</a></h3>
<p>Pour le moment, nous nous sommes intéressés seulement au problème de
régression. Les modèles linéaires que nous avons présentés ne peuvent pas
être appliqués directement aux problèmes de classification : la fonction de
coût et plus spécifiquement le terme calculant l’erreur entre les prédictions
et les valeurs cibles n’est pas adapté.</p>
<p>Avant de présenter comment modifier notre modèle de régression pour qu’il
puisse être utilisé pour des problèmes de classification, nous allons
introduire un jeu de données de classification pour observer les différences
avec le jeu de données de utilisé pour la régression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">donnees</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../datasets/penguins_classification.csv&quot;</span><span class="p">)</span>
<span class="n">donnees</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Longueur Bec (mm)</th>
      <th>Epaisseur Bec (mm)</th>
      <th>Especes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>39.1</td>
      <td>18.7</td>
      <td>Adelie</td>
    </tr>
    <tr>
      <th>1</th>
      <td>39.5</td>
      <td>17.4</td>
      <td>Adelie</td>
    </tr>
    <tr>
      <th>2</th>
      <td>40.3</td>
      <td>18.0</td>
      <td>Adelie</td>
    </tr>
    <tr>
      <th>3</th>
      <td>36.7</td>
      <td>19.3</td>
      <td>Adelie</td>
    </tr>
    <tr>
      <th>4</th>
      <td>39.3</td>
      <td>20.6</td>
      <td>Adelie</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Dans ce jeu de données le but est de prédire l’espèce de pingouin à partir de
la morphologie de son bec (longueur et épaisseur). La particularité est que
l’espèce à prédire est une catégorie et non une valeur numérique continue
comme dans le problème de régression précédent :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">donnees</span><span class="p">[</span><span class="s2">&quot;Especes&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Adelie       151
Gentoo       123
Chinstrap     68
Name: Especes, dtype: int64
</pre></div>
</div>
</div>
</div>
<p>En revanche, les variables prédictives ne sont pas différentes. Donc la
différence entre un problème de classification et régression dépend de du
type de la variable cible.</p>
<p>Ici nous avons trois espèces de pingouins. Ce problème de classification
est dénomé <strong>multi-classe</strong>. Lorsque seulement deux classes sont présentes,
ce problème est appelé <strong>classification binaire</strong>. Nous allons simplifier
notre problème à un problème binaire à des fins didactiques.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">donnees</span> <span class="o">=</span> <span class="n">donnees</span><span class="p">[</span><span class="n">donnees</span><span class="p">[</span><span class="s2">&quot;Especes&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="s2">&quot;Adelie&quot;</span><span class="p">,</span> <span class="s2">&quot;Chinstrap&quot;</span><span class="p">])]</span>
<span class="n">donnees</span><span class="p">[</span><span class="s2">&quot;Especes&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">donnees</span><span class="p">[</span><span class="s2">&quot;Especes&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;category&quot;</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">donnees</span><span class="p">[[</span><span class="s2">&quot;Longueur Bec (mm)&quot;</span><span class="p">,</span> <span class="s2">&quot;Epaisseur Bec (mm)&quot;</span><span class="p">]],</span> <span class="n">donnees</span><span class="p">[</span><span class="s2">&quot;Especes&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Longueur Bec (mm)&quot;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Epaisseur Bec (mm)&quot;</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="s2">&quot;Especes&quot;</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdBu</span><span class="p">,</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_86_0.png" src="../_images/file_01_86_0.png" />
</div>
</div>
<p>Dans ce problème de classification, nous essayons donc de trouver une
séparation entre les deux espèces de pingouins. Il semble que nous pourrions
utiliser une séparation linéaire à cet effet.</p>
<p>En considérant tous les aspects que nous venons de discuter, nous pourrions
modifier notre modèle de régression et faire que les prédictions deviennent
discrète. Une manière est d’utiliser la fonction logistique pour transformer
les prédictions dans un intervalle entre 0 et 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">logistic_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">logistic_function</span><span class="p">(</span><span class="n">xx</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Logistic function&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_89_0.png" src="../_images/file_01_89_0.png" />
</div>
</div>
<p>Notre modèle est donc formuler de la manière suivante :</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = \frac{1}{1 + \exp(-X \beta)}
\]</div>
<p>Donc les valeurs <span class="math notranslate nohighlight">\(\hat{y}\)</span> sont des valeurs entre 0 et 1 et représentent la
probabilité d’appartenir à l’une des deux espèces de pingouins. La fonction
de coût dérivée (incluant les termes pour la régularisation) pour ce problème
est formellement :</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\beta) = \frac{1 - \rho}{2} \|\beta\|_2 + \rho \|\beta\|_1 +
\frac{C}{N} \sum_{i=1}^N \log ( \exp (- y_i (X_i \beta)) + 1)
\]</div>
<p>En pratique, cette fonction de coût est dérivable et il est donc possible
d’optimiser les paramètres <span class="math notranslate nohighlight">\(\beta\)</span> pour minimiser cette fonction de coût.</p>
<p><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> propose un classe <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> pour réaliser cette
optimisation. Cette classe expose un paramètre <code class="docutils literal notranslate"><span class="pre">penalty</span></code> pour choisir le type
de régularisation. Il n’existe donc pas différentes classes pour les
différents types de régularisation.</p>
<p>Il est également important de noter que la régularisation dépend du paramètre
<span class="math notranslate nohighlight">\(C\)</span>. Ce facteur multiplicatif est cependant appliqué au terme calculant
l’erreur et non pas aux termes imposant les contraintes sur les coefficients.
Donc l’impact du paramètre <span class="math notranslate nohighlight">\(C\)</span> dans le modèle <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> est
l’opposé du paramètre <span class="math notranslate nohighlight">\(\alpha\)</span> dans les méthodes de régression. Le paramètre
<span class="math notranslate nohighlight">\(\rho\)</span> permet de choisir entre une regularisation L2 et L1.</p>
<p>Nous allons illustrer ces différents comportements sur le jeu de données de
classification avec une régularisation de type L2.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">helper.plotting</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>

<span class="n">Cs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e5</span><span class="p">]</span>

<span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">Cs</span><span class="p">:</span>
    <span class="n">modele</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">)</span>
    <span class="n">modele</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
        <span class="n">modele</span><span class="p">,</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;decision_function&quot;</span><span class="p">,</span>
        <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdBu</span><span class="p">,</span>
        <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;pcolormesh&quot;</span><span class="p">,</span>
        <span class="n">shading</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Longueur Bec (mm)&quot;</span><span class="p">,</span>
        <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Epaisseur Bec (mm)&quot;</span><span class="p">,</span>
        <span class="n">c</span><span class="o">=</span><span class="s2">&quot;Especes&quot;</span><span class="p">,</span>
        <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdBu</span><span class="p">,</span>
        <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Séparation avec </span><span class="se">\n</span><span class="si">{</span><span class="n">C</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">modele</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coefficients avec </span><span class="si">{</span><span class="n">C</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_91_0.png" src="../_images/file_01_91_0.png" />
<img alt="../_images/file_01_91_1.png" src="../_images/file_01_91_1.png" />
<img alt="../_images/file_01_91_2.png" src="../_images/file_01_91_2.png" />
<img alt="../_images/file_01_91_3.png" src="../_images/file_01_91_3.png" />
</div>
</div>
<p>Nous pouvons donc confirmer que plus <span class="math notranslate nohighlight">\(C\)</span> est petit, plus les coefficients
sont contraints et donc petits. Nous pouvons également observer, de manière
intuitive, l’effet de rendre un coefficient nul (ou proche de zero) : la
séparation devient perpendiculaire à un des axes correspondant au coefficient
non-nul.</p>
</div>
<div class="section" id="alternative-en-utilisant-une-approche-geometrique">
<h3>Alternative en utilisant une approche géométrique<a class="headerlink" href="#alternative-en-utilisant-une-approche-geometrique" title="Permalink to this headline">¶</a></h3>
<p>Un modèle alternatif aux modèles précédent est connu sous le nom de <strong>Support
Vector Machine (SVM)</strong>. Le problème d’optimisation a été formulé de façcon
différente, en utilisant une approche géométrique.</p>
<p>Nous allons essayer de l’illustrer de manière intuitive. Pour cela, nous
allons utiliser un jeu de données synthétique.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
<span class="n">donnees</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;Colonne #0&quot;</span><span class="p">:</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="s2">&quot;Colonne #1&quot;</span><span class="p">:</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="s2">&quot;Cible&quot;</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="n">donnees</span><span class="p">[</span><span class="s2">&quot;Cible&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">donnees</span><span class="p">[</span><span class="s2">&quot;Cible&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;category&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Colonne #0&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Colonne #1&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;Cible&quot;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdBu</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:xlabel=&#39;Colonne #0&#39;, ylabel=&#39;Colonne #1&#39;&gt;
</pre></div>
</div>
<img alt="../_images/file_01_94_1.png" src="../_images/file_01_94_1.png" />
</div>
</div>
<p>Ce jeu de données est simple puisqu’il est composé de deux nuages de points
qui peuvent être séparés facilement en utilisant une séparation linéaire.</p>
<p>Le terme “Support Vectors” réfère à certains échantillons qui seront
sélectionnés permettant de définir la séparation linéaire. Nous allons
entraîner un modèle SVM sur ce jeu de données et expliquer les intuitions
derrière l’optimisation réalisée.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="n">modele</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">modele</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SVC(kernel=&#39;linear&#39;, random_state=0)
</pre></div>
</div>
</div>
</div>
<p>Maintenant, nous allons afficher la séparation linéaire ainsi que les
échantillons qui constituent les vecteurs support.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">display</span> <span class="o">=</span> <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
    <span class="n">modele</span><span class="p">,</span>
    <span class="n">X</span><span class="p">,</span>
    <span class="n">response_method</span><span class="o">=</span><span class="s2">&quot;decision_function&quot;</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdBu</span><span class="p">,</span>
    <span class="n">plot_method</span><span class="o">=</span><span class="s2">&quot;contour&quot;</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
    <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">display</span><span class="o">.</span><span class="n">surface_</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">donnees</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s2">&quot;Colonne #0&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Colonne #1&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;Cible&quot;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdBu</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">modele</span><span class="o">.</span><span class="n">support_</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">modele</span><span class="o">.</span><span class="n">support_</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Vecteurs support&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/file_01_98_0.png" src="../_images/file_01_98_0.png" />
</div>
</div>
<p>Nous pouvons remarquer que les vecteurs support sont les plus proches de la
séparation linéaire. En effet, il définisse cette séparation : la séparation
correspond à l’hyperplan (ici une ligne) de manière à ce que la distance
entre la séparation et ces points soit maximale. Cette optimisation est
souvent appelée la <strong>maximisation de la marge</strong> ou la marge est l’espace
défini entre les échantillons et la séparation.</p>
<p>Ce problème est bien entendu relié à une certaine fonction de coût connu sous
le nom de fonction de Hinge et formulée comme suit :</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\beta) = \|\beta\|^2 + \frac{C}{N} \sum_{i=1}^N \max(0, 1 - y_i
X_i \beta)
\]</div>
<p>Il est intéressant de noter que les prédictions ici n’ont pas de sens
probabilistes : un échantillon est affecté à une classe suivant de quel coté
il se trouve de la séparation. De plus, nous pouvons connaître la distance à
cet hyperplan mais il nous impossible de calculer une probabilité
d’appartenance à un classe.</p>
<p>En pratique, il existe une astuce où une fonction sigmoid est utilisée a
posteriori pour déterminer la probabilité d’appartenance à une classe. Cette
approche est connu sous le nom de la calibration de Platt.</p>
</div>
<div class="section" id="note-concernant-le-parametre-de-regularisation">
<h3>Note concernant le paramètre de régularisation<a class="headerlink" href="#note-concernant-le-parametre-de-regularisation" title="Permalink to this headline">¶</a></h3>
<p>Dans cette section, nous voulons attirer l’attention de notre lecteur sur le
choix de la paramètre de régularisation. Dans la section précédente, nous
avons seulement montrer de manière expérimentale l’effet des paramètres <span class="math notranslate nohighlight">\(C\)</span>
et <span class="math notranslate nohighlight">\(\alpha\)</span>. En revanche, nous n’avons pas expliqué quelles valeurs nous
devrions assigner à ce paramètre de régularisation.</p>
<p>En réalité, ce paramètre ne peut pas être choisi de manière arbitraire. En
d’autres mots, il dépendra du problème de régression ou classification à
résoudre. Il est donc nécessaire de rechercher la meilleur valeur du
paramètre de régularisation après avoir entraîné et évalué plusieurs modèles.</p>
<p>Nous reviendrons sur ce sujet dénomé <strong>recherche d’hyperparamètres</strong> et qui
requiert une présentation exhaustive.</p>
</div>
<div class="section" id="importance-du-pretraitement">
<h3>Importance du prétraitement<a class="headerlink" href="#importance-du-pretraitement" title="Permalink to this headline">¶</a></h3>
<div class="section" id="effet-des-donnees-non-traitees">
<h4>Effet des données non-traîtées<a class="headerlink" href="#effet-des-donnees-non-traitees" title="Permalink to this headline">¶</a></h4>
<p>Pour conclure sur la présentation des modèles linéaires, nous allons nous
intéresser à la question de prétraitement et son effet sur l’optimisation.
Le prétraitement est particulièrement important avec les algorithmes
itératifs basés sur le gradient.</p>
<p>Nous allons l’illustrer sur un problème de classification en utilisant
le modèle de <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">donnees</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../datasets/adult-census-numeric-all.csv&quot;</span><span class="p">)</span>
<span class="n">colonne_cible</span> <span class="o">=</span> <span class="s2">&quot;class&quot;</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">donnees</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">colonne_cible</span><span class="p">),</span> <span class="n">donnees</span><span class="p">[</span><span class="n">colonne_cible</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>A des fins d’évaluation, nous allons seulement utiliser un séparation simple
afin d’obtenir un jeu de données pour d’entraînement et un autre pour le
test.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Dans les sections précédentes, nous avons utilisé un modèle de la manière
suivante :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">modele</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">debut</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">modele</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">temps_entrainement</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">debut</span>
<span class="n">modele</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8117271312750799
</pre></div>
</div>
</div>
</div>
<p>Avec cette approche, nous avons directement entraîné notre modèle sur les
données originales. Nous pouvon vérifier le temps pris par l’algorithme
pour trouver les meilleurs coefficients. Nous pouvons également vérifier
le nombre d’itérations de l’algorithme.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Temps d&#39;entrainement : </span><span class="si">{</span><span class="n">temps_entrainement</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> secondes&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Nombre d&#39;itérations : </span><span class="si">{</span><span class="n">modele</span><span class="o">.</span><span class="n">n_iter_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">modele</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">barh</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Temps d&#39;entrainement : 0.34 secondes
Nombre d&#39;itérations : [100]
</pre></div>
</div>
<img alt="../_images/file_01_107_1.png" src="../_images/file_01_107_1.png" />
</div>
</div>
<p>L’algorithme utilisé pour optimiser les coefficients est basé sur le gradient
de la fonction de coût. Dans ce contexte, l’espace de cette fonction de coût
est particulièrement important. Nous pouvons nous attarder sur une
statistique tel que la moyenne et l’écart-type des différentes variables
prédictives.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">aggregate</span><span class="p">([</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;std&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>education-num</th>
      <th>capital-gain</th>
      <th>capital-loss</th>
      <th>hours-per-week</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>mean</th>
      <td>38.685458</td>
      <td>10.073271</td>
      <td>1063.206929</td>
      <td>86.779831</td>
      <td>40.422484</td>
    </tr>
    <tr>
      <th>std</th>
      <td>13.730130</td>
      <td>2.571369</td>
      <td>7374.547590</td>
      <td>401.559034</td>
      <td>12.425426</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Nous pouvons observer que les données ne sont pas centrées car la moyenne des
différentes variables prédictives est différente. Ceci n’est pas forcément le
plus problématique. En revanche, les écart-types sont très différents. Ceci
signifie que les coefficients seront donc très différents également. Cela
sera un problème lors de l’estimation du gradient car des coefficients de
plus grandes valeurs domineront la descente du gradient et freinerons
l’algorithme. Il est donc possible de normaliser les données pour quelles
soient centrées et dans le même ordre de grandeur. Ceci aura donc un impact
sur la valeur des coefficients trouvés.</p>
<p>La normalisation consiste donc à retrancher la moyenne de chaque variable
prédictive et de diviser par l’écart-type. En revanche, pour ne pas utiliser
d’information du jeu de test, il est important de calculer les statistiques
sur le jeu de données d’entraînement seulement.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_mean</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">X_train_std</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_normalise</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span> <span class="o">-</span> <span class="n">X_train_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">X_train_std</span>
<span class="n">X_test_normalise</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_test</span> <span class="o">-</span> <span class="n">X_train_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">X_train_std</span>
</pre></div>
</div>
</div>
</div>
<p>Nous pouvons donc maintenant observer que nous avons des données avec une
moyenne nulle et un écart-type unitaire.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_normalise</span><span class="o">.</span><span class="n">aggregate</span><span class="p">([</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;std&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>education-num</th>
      <th>capital-gain</th>
      <th>capital-loss</th>
      <th>hours-per-week</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>mean</th>
      <td>-7.177003e-17</td>
      <td>2.211293e-16</td>
      <td>3.724283e-17</td>
      <td>2.676828e-17</td>
      <td>8.146869e-18</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.000000e+00</td>
      <td>1.000000e+00</td>
      <td>1.000000e+00</td>
      <td>1.000000e+00</td>
      <td>1.000000e+00</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test_normalise</span><span class="o">.</span><span class="n">aggregate</span><span class="p">([</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;std&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>education-num</th>
      <th>capital-gain</th>
      <th>capital-loss</th>
      <th>hours-per-week</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>mean</th>
      <td>-0.012198</td>
      <td>0.007493</td>
      <td>0.008603</td>
      <td>0.007196</td>
      <td>-0.000033</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.994257</td>
      <td>0.999403</td>
      <td>1.041399</td>
      <td>1.014344</td>
      <td>0.989056</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Nous pouvons réitérer l’expérience précédente et observer l’effet sur les
performance de l’algorithme d’optimisation mais également sur la valeur des
coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">debut</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">modele</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_normalise</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">temps_entrainement</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">debut</span>
<span class="n">modele</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_normalise</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8118909180247318
</pre></div>
</div>
</div>
</div>
<p>En ce qui concerne les performances statistiques, notre modèle obtient
un score similaire.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Temps d&#39;entrainement : </span><span class="si">{</span><span class="n">temps_entrainement</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> secondes&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Nombre d&#39;itérations : </span><span class="si">{</span><span class="n">modele</span><span class="o">.</span><span class="n">n_iter_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">modele</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">barh</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Temps d&#39;entrainement : 0.11 secondes
Nombre d&#39;itérations : [13]
</pre></div>
</div>
<img alt="../_images/file_01_119_1.png" src="../_images/file_01_119_1.png" />
</div>
</div>
<p>En revanche, le temps pour effectuer l’entraînement ainsi que le nombre
d’itérations sont plus failbles. En outre, nous pouvons observer que les
valeurs des coefficients sont plus proches car nous opérons sur des données
qui sont dans le même ordre de grandeur.</p>
</div>
<div class="section" id="pretraitement-integre-dans-scikit-learn">
<h4>Prétraitement intégré dans <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code><a class="headerlink" href="#pretraitement-integre-dans-scikit-learn" title="Permalink to this headline">¶</a></h4>
<p>Dans la procédure que nous avons dévoilé dans la section précédente, nous
manuellement créer un jeu de données qui ensuite a été délégué au modèle
<code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>. Cette opération est très souvent réalisée. A cet essort,
<code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> propose le concept de “pipeline” qui permet d’enchainer des
étapes de prétraitement successives finissant par l’entraînement d’un modèle
prédictif. De plus, ce pipeline expose la même interface de programmation que
les modèles que nous avons déjà utilisés (i.e. <code class="docutils literal notranslate"><span class="pre">fit</span></code> + <code class="docutils literal notranslate"><span class="pre">predict</span></code>).</p>
<p>Dans cette section, nous allons brièvement présenter l’interface des classes
de prétraitement disponibles dans <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>. Par la suite, nous
introduirons les pipelines ce qui nous permettra d’effectuer des
prétraitements suivient d’un apprentrissage de modèle prédictif.</p>
<p><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> propose une classes permettant d’effectuer le centrage
et la normalisation des données.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>StandardScaler()
</pre></div>
</div>
</div>
</div>
<p>L’appelle à la méthode <code class="docutils literal notranslate"><span class="pre">fit</span></code> permet de calculer les paramètres de
normalisation : la moyenne et l’écart-type.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Les moyennes par variable sont</span><span class="se">\n</span><span class="si">{</span><span class="n">scaler</span><span class="o">.</span><span class="n">mean_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Les écart-types par variable sont</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">scaler</span><span class="o">.</span><span class="n">scale_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Les moyennes par variable sont
[  38.68545767   10.07327127 1063.20692856   86.77983129   40.42248369]
Les écart-types par variable sont
 [1.37299421e+01 2.57133407e+00 7.37444693e+03 4.01553553e+02
 1.24252559e+01]
</pre></div>
</div>
</div>
</div>
<p>La méthode <code class="docutils literal notranslate"><span class="pre">transform</span></code> permet de normaliser les données en utilisant les
statistiques calculées lors de l’appel à la méthode <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_normalise</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Cette opération est donc équivalente à l’opération manuelle que nous avons
déjà vu. En revanche, il est important d’observer que <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> peut
consommer des dataframes, mais que celles-ci seront automatiquement
converties en matrices <code class="docutils literal notranslate"><span class="pre">numpy</span></code>. De pus, le résultat d’une opération comme
<code class="docutils literal notranslate"><span class="pre">transform</span></code> nous donnera une matrice <code class="docutils literal notranslate"><span class="pre">numpy</span></code> en sortie. La raison est que les
méthodes utilisées dans <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> requiert de l’optmisation numérique
pour lequel <code class="docutils literal notranslate"><span class="pre">numpy</span></code> est plus adapté que <code class="docutils literal notranslate"><span class="pre">pandas</span></code>.</p>
<p>Pour l’instant, nous n’avons pas encore utilisé les pipelines. Cette classe
est une pièce maîtresse dans <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> et est généralement ignoré.
Un pipeline permet de créer une séquence d’opération de type transformation
suivi d’un apprentissage de modèle prédictif. Nous allons le mettre en
pratique sur notre exemple précédent.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn</span>

<span class="c1"># pour obtenir des diagrams pour les pipelines</span>
<span class="n">sklearn</span><span class="o">.</span><span class="n">set_config</span><span class="p">(</span><span class="n">display</span><span class="o">=</span><span class="s2">&quot;diagram&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">modele</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span>
    <span class="n">steps</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;normalisation&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s2">&quot;classification&quot;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">()),</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">modele</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e {color: black;background-color: white;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e pre{padding: 0;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-toggleable {background-color: white;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-estimator:hover {background-color: #d4ebff;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-item {z-index: 1;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-parallel::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-parallel-item:only-child::after {width: 0;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e div.sk-container {display: inline-block;position: relative;}</style><div id="sk-1b96d806-2dc7-4185-9831-adccbbdf2e2e" class"sk-top-container"><div class="sk-container"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="5e8ba4c4-4039-4763-8e2c-ddd52bf22451" type="checkbox" ><label class="sk-toggleable__label" for="5e8ba4c4-4039-4763-8e2c-ddd52bf22451">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[('normalisation', StandardScaler()),
                ('classification', LogisticRegression())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="e6771664-604c-4e7c-93b7-44bf0b92ff43" type="checkbox" ><label class="sk-toggleable__label" for="e6771664-604c-4e7c-93b7-44bf0b92ff43">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="fa0a2039-1ae0-48c5-8cf6-0f8df8b4b2d6" type="checkbox" ><label class="sk-toggleable__label" for="fa0a2039-1ae0-48c5-8cf6-0f8df8b4b2d6">LogisticRegression</label><div class="sk-toggleable__content"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div>
</div>
<p>Ce nouveau modèle prédictif expose la même interface que l’estimateur
<code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code>. Lorsque que la méthode <code class="docutils literal notranslate"><span class="pre">fit</span></code> est appelée, le
<code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> appèlera ca propre méthode <code class="docutils literal notranslate"><span class="pre">fit</span></code> et transformera les données
d’entrée. Ensuite, le modèle sera entraîné sur ces données prétraitées.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modele</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-556a585a-ee23-4ae7-a326-43815f22be4f {color: black;background-color: white;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f pre{padding: 0;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-toggleable {background-color: white;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-estimator:hover {background-color: #d4ebff;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-item {z-index: 1;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-parallel::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-parallel-item:only-child::after {width: 0;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-556a585a-ee23-4ae7-a326-43815f22be4f div.sk-container {display: inline-block;position: relative;}</style><div id="sk-556a585a-ee23-4ae7-a326-43815f22be4f" class"sk-top-container"><div class="sk-container"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="1bc58a7a-827d-4569-8283-8d198af5b5c8" type="checkbox" ><label class="sk-toggleable__label" for="1bc58a7a-827d-4569-8283-8d198af5b5c8">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[('normalisation', StandardScaler()),
                ('classification', LogisticRegression())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="e0a0e01a-35ca-4c07-915d-37125c20ab1c" type="checkbox" ><label class="sk-toggleable__label" for="e0a0e01a-35ca-4c07-915d-37125c20ab1c">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="38ef022b-a0a4-4827-ba57-bc528301956f" type="checkbox" ><label class="sk-toggleable__label" for="38ef022b-a0a4-4827-ba57-bc528301956f">LogisticRegression</label><div class="sk-toggleable__content"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div>
</div>
<p>Nous pouvons donc inspecter chacun des estimateurs entraînés dans le
pipeline.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Les moyennes par variable sont</span><span class="se">\n</span><span class="si">{</span><span class="n">modele</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Les écart-types par variable sont</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">modele</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scale_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Les moyennes par variable sont
[  38.68545767   10.07327127 1063.20692856   86.77983129   40.42248369]
Les écart-types par variable sont
 [1.37299421e+01 2.57133407e+00 7.37444693e+03 4.01553553e+02
 1.24252559e+01]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Nombre d&#39;itérations : </span><span class="si">{</span><span class="n">modele</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">n_iter_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">modele</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">barh</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Nombre d&#39;itérations : [13]
</pre></div>
</div>
<img alt="../_images/file_01_133_1.png" src="../_images/file_01_133_1.png" />
</div>
</div>
<p>Finalement, ce pipeline peut-être utilisé pour prédire. Dans un premier
temps, les données seront normalisées et seront ensuite données à la méthode
de classification qui appèlera la méthode <code class="docutils literal notranslate"><span class="pre">predict</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">modele</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8118909180247318
</pre></div>
</div>
</div>
</div>
<p>Nous pouvons donc même utiliser ce type de pipeline dans une validation
croisée.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">modele</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cv_results</span><span class="p">)</span>
<span class="n">cv_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.113776</td>
      <td>0.020821</td>
      <td>0.812161</td>
      <td>0.814706</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.117651</td>
      <td>0.019968</td>
      <td>0.809602</td>
      <td>0.815243</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.111480</td>
      <td>0.021072</td>
      <td>0.813370</td>
      <td>0.814429</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.111407</td>
      <td>0.021554</td>
      <td>0.813268</td>
      <td>0.814429</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.111235</td>
      <td>0.021569</td>
      <td>0.822072</td>
      <td>0.812484</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="probleme-de-convergence">
<h4>Problème de convergence<a class="headerlink" href="#probleme-de-convergence" title="Permalink to this headline">¶</a></h4>
<p>Nous avons donc montré dans les sections précédentes que le prétraitement
des données peut accélérer l’apprentissage de modèle prédictif. Il est même
possible qu’un modèle prédictif n’est pas forcément le temps de converger
dans le budget d’itération donné. Nous allons illuster ces propos :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modele</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">modele</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre></div>
</div>
<div class="output text_html"><style>#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 {color: black;background-color: white;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 pre{padding: 0;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-toggleable {background-color: white;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-item {z-index: 1;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-parallel::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-parallel-item:only-child::after {width: 0;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-c4366c3f-a19e-429a-90a8-c73cd181aff4 div.sk-container {display: inline-block;position: relative;}</style><div id="sk-c4366c3f-a19e-429a-90a8-c73cd181aff4" class"sk-top-container"><div class="sk-container"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="5a6c4a4c-48bc-4374-b95a-24411025f233" type="checkbox" checked><label class="sk-toggleable__label" for="5a6c4a4c-48bc-4374-b95a-24411025f233">LogisticRegression</label><div class="sk-toggleable__content"><pre>LogisticRegression(max_iter=20)</pre></div></div></div></div></div></div></div>
</div>
<p>Nous pouvons observer un warning de type <code class="docutils literal notranslate"><span class="pre">ConvergenceWarning</span></code>. Cela signifie
que nous n’avons pas trouver le minimum de notre fonction de coût. Sur le
problème que nous avons maintenant, nous avons pu voir que 20 itérations sont
suffisantes pour converger, si nous prétraitons les données à l’avance. Ici,
la solution serait donc de normaliser nos données. En revanche, il peut
arriver qu’un algorithme est besoin de plus de temps pour converger. Dans ce
cas, il sera donc nécessaire d’augmenter le nombre d’itérations en augmentant
la valeur du paramètre <code class="docutils literal notranslate"><span class="pre">max_iter</span></code>.</p>
</div>
</div>
</div>
<div class="section" id="resolution-de-problemes-non-lineaires">
<h2>Résolution de problèmes non-linéaires<a class="headerlink" href="#resolution-de-problemes-non-lineaires" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="quantifier-l-incertitude-des-predictions">
<h2>Quantifier l’incertitude des prédictions<a class="headerlink" href="#quantifier-l-incertitude-des-predictions" title="Permalink to this headline">¶</a></h2>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapter_00"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="file_00.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Introduction</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="file_02.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Famille des modèles non-paramétriques</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Cédric Lemaître<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>