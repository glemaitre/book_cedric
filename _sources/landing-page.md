# Table of content

Partie machine learning avec Scikit-learn :

## Apprentissage supervisée
### Présentation générale
### Méthodologie basée sur les modèles linéaires
Ecrit
#### Les moindres carrés
Ecrit
#### Hubert Regression
#### Quantile Regression
#### Approche proba -> bayésienne
#### Régression d’arête
Ecrit mais mettre en accord avec la suite
#### Lasso
#### Elastic-Net
#### OMP
#### Régression logistique
#### Descente de gradient stochastic
#### Perceptron
#### SVM
##### Présentation
##### Problème géométrique
##### Refit sig sur hyperplan -> estimation de proba car calcul de distance - Régression
##### Classification
##### Les fonctions noyaux
##### Exemple sur un exemple jouet et sur un jeu de données de la vraie vie

#### Les plus proches voisins
##### Classification
##### Régression

#### Processus Gaussian
##### Regression
##### Classification
##### Les fonctions à noyaux

#### Arbres de décisions (gradient boosting + random forest)
##### Présentations générales
##### Classification
##### Régressions

#### Les réseaux de neurones
##### Présentation générale
##### Perceptron multi-couche
##### Classification
##### Régression
##### Régularisation


## Apprentissage non-supervisé
### Modèle de mélanges gaussiens
### Apprentissage de type manifold
### Méthodes de partitionnement (clustering)
#### K-means
#### Mean Shift
#### Méthode spectral
#### Méthode hiérarchique
#### Le Bi-clustering
### Méthodes utilisant la décomposition du signal
#### PCA / reduction dimension
#### TSNE
#### NMF
#### Embedding Réseaux de neurone

### ICA
### LDA
### Réseaux de neurones


## Recherche de paramètre optimaux d’un modèle
Calcul et traitement de données de grande dimension / problème réel


### La notion de « pipeline »
### Pré-traitement des données
### Cas des données manquantes